{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Advanced Data Mining","title":"Advanced Data Mining"},{"location":"#advanced-data-mining","text":"","title":"Advanced Data Mining"},{"location":"Change Detection/change_detection/","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from matplotlib import rcParams rcParams['figure.figsize'] = 11.7,8.27 sns.set() In order to detect change between two windows of a stream S, we test if the P(x) in the current window is different from the P(x) in the previous window Drift has occurred if P(x)ti != P(x)ti+1 To determine if the change in the observed P(x) is the sign of a drift, and that it is not just due to chance, a significance test can be used. Kolmogorov-Smirnov Test Given below are the observed frequencies of grades obtained by a sample of OVGU students in 2018 and 2019. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 18, 13, 12, 7, 9, 3, 12, 2], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 18 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 2 Tirtha believes that the grades of the students have improved from last year (drift). However, Vishnu is skeptical and suspects that the shift in grades is very small and not significant enough to conclude that anything has improved. The Kolmogorov-Smirnov Test can help them determine who is right. KS Test Steps: 1) Calculate the CDFs of both the distributions 2) Find the maximum absolute difference max|D| between the two CDFS 3) Compare max|D| with the critical value at a desired alpha obtained from the KS table. 4) Conclude that the change is significant if max|D| > critical value grades['proportion (2018)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['proportion (2019)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) Grade 1.0 9 4 0.09 0.040816 1.3 5 18 0.05 0.183673 1.7 12 18 0.12 0.183673 2.0 18 13 0.18 0.132653 2.3 16 12 0.16 0.122449 2.7 12 7 0.12 0.071429 3.0 15 9 0.15 0.091837 3.3 5 3 0.05 0.030612 3.7 2 12 0.02 0.122449 4.0 6 2 0.06 0.020408 The CDFs and their absolute differences are calculated below grades['cdf (2018)'] = grades['proportion (2018)'].cumsum() grades['cdf (2019)'] = grades['proportion (2019)'].cumsum() grades['D'] = grades.apply(lambda x: np.round(np.abs(x['cdf (2018)'] - x['cdf (2019)']), 2), axis=1) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) cdf (2018) cdf (2019) D Grade 1.0 9 4 0.09 0.040816 0.09 0.040816 0.05 1.3 5 18 0.05 0.183673 0.14 0.224490 0.08 1.7 12 18 0.12 0.183673 0.26 0.408163 0.15 2.0 18 13 0.18 0.132653 0.44 0.540816 0.10 2.3 16 12 0.16 0.122449 0.60 0.663265 0.06 2.7 12 7 0.12 0.071429 0.72 0.734694 0.01 3.0 15 9 0.15 0.091837 0.87 0.826531 0.04 3.3 5 3 0.05 0.030612 0.92 0.857143 0.06 3.7 2 12 0.02 0.122449 0.94 0.979592 0.04 4.0 6 2 0.06 0.020408 1.00 1.000000 0.00 Below is a visualization of the CDFs and their absolute differences sns.lineplot(data=grades, y=\"cdf (2018)\", x=grades.index) sns.lineplot(data=grades, y=\"cdf (2019)\", x=grades.index) def plot_diff_line(index, row): plt.plot([index, index], [row['cdf (2019)'], row['cdf (2018)']], color='r', linestyle='-', linewidth=2) plt.ylabel(\"Probability\") for index, row in grades.iterrows(): plot_diff_line(index, row) plt.annotate('Max Difference', xy=(1.7, 0.3), xytext=(2, 0.35), arrowprops=dict(facecolor='black', shrink=0.05) ) Text(2,0.35,'Max Difference') The Max|D| between the two CDFs is 0.15 From the KS table, the critical value at alpha 0.05 is 1.36/root(n) = 0.136 Since Max|D| > critical value, with 95% confidence, we reject the null hypothesis that the two distributions do not differ, which means we can say that OVGU grades have improved. Tirtha was right. However, Vishnu contests this and says that 95% confidence isn't good enough. He recommends that they be 99% confident before making such a claim about the improvement in grades. So, they look at the KS table again, and they get the critical value at alpha 0.01, which is 1.63/root(n) = 0.163 This time, Max|D| < critical value with 99%; therefore, with 99% confidence, we fail to reject the null hypothesis that the two distributions do not differ, which means that the shift in grades might be due to chance, and the distribution might not have drifted Finding the distance between two probability distributions: Kulback-Leibler Divergence This is a measure to calculate the distance between two probability distributions. Note: this isn't a distance metric because it violates the symmetry and triangle inequality properties of distance metrics. We will use the same grade distributions from earlier. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 18, 13, 12, 7, 9, 3, 12, 2], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 18 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 2 The formula for the KL Divergence is The $ part is a ratio. Therefore, if the two distributions P and Q are almost identical, the probability of x in distribution P will be almost equal to the probability of x in distribution Q, so the ratio will be close to 1. Since the log of a number close to 1 is close to 0, summing multiple numbers close to 0 will result in a low KL Divergence. def kl_divergence(P, Q): kl = 0 for i in range(len(P)): kl += P[i] * np.log(P[i]/Q[i]) return np.round(kl, 3) Steps to calculate KL Divergence for discrete data: 1) Calculate the probabilities for the two distributions from the data 2) Apply the formula # Calculate probability distribution grades['P(x)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['Q(x)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) display(grades) ax = sns.lineplot(data=grades, x=grades.index, y=\"P(x)\", label=\"P(x)\") ax = sns.lineplot(data=grades, x=grades.index, y=\"Q(x)\", label=\"Q(x)\") ax.set(ylabel='Probability', xlabel='Grade') plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 P(x) Q(x) Grade 1.0 9 4 0.09 0.040816 1.3 5 18 0.05 0.183673 1.7 12 18 0.12 0.183673 2.0 18 13 0.18 0.132653 2.3 16 12 0.16 0.122449 2.7 12 7 0.12 0.071429 3.0 15 9 0.15 0.091837 3.3 5 3 0.05 0.030612 3.7 2 12 0.02 0.122449 4.0 6 2 0.06 0.020408 px = grades['P(x)'].to_numpy() qx = grades['Q(x)'].to_numpy() print(kl_divergence(px, qx)) print(kl_divergence(qx, px)) 0.242 0.314 The KL Divergence between P and Q is 0.242 The KL Divergence between Q and P is 0.314","title":"Change detection"},{"location":"Change Detection/change_detection/#kolmogorov-smirnov-test","text":"Given below are the observed frequencies of grades obtained by a sample of OVGU students in 2018 and 2019. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 18, 13, 12, 7, 9, 3, 12, 2], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 18 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 2 Tirtha believes that the grades of the students have improved from last year (drift). However, Vishnu is skeptical and suspects that the shift in grades is very small and not significant enough to conclude that anything has improved. The Kolmogorov-Smirnov Test can help them determine who is right. KS Test Steps: 1) Calculate the CDFs of both the distributions 2) Find the maximum absolute difference max|D| between the two CDFS 3) Compare max|D| with the critical value at a desired alpha obtained from the KS table. 4) Conclude that the change is significant if max|D| > critical value grades['proportion (2018)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['proportion (2019)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) Grade 1.0 9 4 0.09 0.040816 1.3 5 18 0.05 0.183673 1.7 12 18 0.12 0.183673 2.0 18 13 0.18 0.132653 2.3 16 12 0.16 0.122449 2.7 12 7 0.12 0.071429 3.0 15 9 0.15 0.091837 3.3 5 3 0.05 0.030612 3.7 2 12 0.02 0.122449 4.0 6 2 0.06 0.020408","title":"Kolmogorov-Smirnov Test"},{"location":"Change Detection/change_detection/#the-cdfs-and-their-absolute-differences-are-calculated-below","text":"grades['cdf (2018)'] = grades['proportion (2018)'].cumsum() grades['cdf (2019)'] = grades['proportion (2019)'].cumsum() grades['D'] = grades.apply(lambda x: np.round(np.abs(x['cdf (2018)'] - x['cdf (2019)']), 2), axis=1) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) cdf (2018) cdf (2019) D Grade 1.0 9 4 0.09 0.040816 0.09 0.040816 0.05 1.3 5 18 0.05 0.183673 0.14 0.224490 0.08 1.7 12 18 0.12 0.183673 0.26 0.408163 0.15 2.0 18 13 0.18 0.132653 0.44 0.540816 0.10 2.3 16 12 0.16 0.122449 0.60 0.663265 0.06 2.7 12 7 0.12 0.071429 0.72 0.734694 0.01 3.0 15 9 0.15 0.091837 0.87 0.826531 0.04 3.3 5 3 0.05 0.030612 0.92 0.857143 0.06 3.7 2 12 0.02 0.122449 0.94 0.979592 0.04 4.0 6 2 0.06 0.020408 1.00 1.000000 0.00","title":"The CDFs and their absolute differences are calculated below"},{"location":"Change Detection/change_detection/#below-is-a-visualization-of-the-cdfs-and-their-absolute-differences","text":"sns.lineplot(data=grades, y=\"cdf (2018)\", x=grades.index) sns.lineplot(data=grades, y=\"cdf (2019)\", x=grades.index) def plot_diff_line(index, row): plt.plot([index, index], [row['cdf (2019)'], row['cdf (2018)']], color='r', linestyle='-', linewidth=2) plt.ylabel(\"Probability\") for index, row in grades.iterrows(): plot_diff_line(index, row) plt.annotate('Max Difference', xy=(1.7, 0.3), xytext=(2, 0.35), arrowprops=dict(facecolor='black', shrink=0.05) ) Text(2,0.35,'Max Difference') The Max|D| between the two CDFs is 0.15 From the KS table, the critical value at alpha 0.05 is 1.36/root(n) = 0.136 Since Max|D| > critical value, with 95% confidence, we reject the null hypothesis that the two distributions do not differ, which means we can say that OVGU grades have improved. Tirtha was right. However, Vishnu contests this and says that 95% confidence isn't good enough. He recommends that they be 99% confident before making such a claim about the improvement in grades. So, they look at the KS table again, and they get the critical value at alpha 0.01, which is 1.63/root(n) = 0.163 This time, Max|D| < critical value with 99%; therefore, with 99% confidence, we fail to reject the null hypothesis that the two distributions do not differ, which means that the shift in grades might be due to chance, and the distribution might not have drifted","title":"Below is a visualization of the CDFs and their absolute differences"},{"location":"Change Detection/change_detection/#finding-the-distance-between-two-probability-distributions-kulback-leibler-divergence","text":"This is a measure to calculate the distance between two probability distributions. Note: this isn't a distance metric because it violates the symmetry and triangle inequality properties of distance metrics. We will use the same grade distributions from earlier. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 18, 13, 12, 7, 9, 3, 12, 2], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 18 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 2 The formula for the KL Divergence is The $ part is a ratio. Therefore, if the two distributions P and Q are almost identical, the probability of x in distribution P will be almost equal to the probability of x in distribution Q, so the ratio will be close to 1. Since the log of a number close to 1 is close to 0, summing multiple numbers close to 0 will result in a low KL Divergence. def kl_divergence(P, Q): kl = 0 for i in range(len(P)): kl += P[i] * np.log(P[i]/Q[i]) return np.round(kl, 3) Steps to calculate KL Divergence for discrete data: 1) Calculate the probabilities for the two distributions from the data 2) Apply the formula # Calculate probability distribution grades['P(x)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['Q(x)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) display(grades) ax = sns.lineplot(data=grades, x=grades.index, y=\"P(x)\", label=\"P(x)\") ax = sns.lineplot(data=grades, x=grades.index, y=\"Q(x)\", label=\"Q(x)\") ax.set(ylabel='Probability', xlabel='Grade') plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 P(x) Q(x) Grade 1.0 9 4 0.09 0.040816 1.3 5 18 0.05 0.183673 1.7 12 18 0.12 0.183673 2.0 18 13 0.18 0.132653 2.3 16 12 0.16 0.122449 2.7 12 7 0.12 0.071429 3.0 15 9 0.15 0.091837 3.3 5 3 0.05 0.030612 3.7 2 12 0.02 0.122449 4.0 6 2 0.06 0.020408 px = grades['P(x)'].to_numpy() qx = grades['Q(x)'].to_numpy() print(kl_divergence(px, qx)) print(kl_divergence(qx, px)) 0.242 0.314","title":"Finding the distance between two probability distributions: Kulback-Leibler Divergence"},{"location":"Change Detection/change_detection/#the-kl-divergence-between-p-and-q-is-0242","text":"","title":"The KL Divergence between P and Q is 0.242"},{"location":"Change Detection/change_detection/#the-kl-divergence-between-q-and-p-is-0314","text":"","title":"The KL Divergence between Q and P is 0.314"},{"location":"Clustream/clustream/","text":"Clustream import pandas as pd import math import numpy as np from collections import Counter from scipy.spatial import distance from sklearn.cluster import KMeans import seaborn as sns import matplotlib.pyplot as plt sns.set() from IPython.display import display_html def display_side_by_side(*args): html_str='' for df in args: html_str+=df.to_html() display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True) import warnings warnings.filterwarnings('ignore') file = 'C:\\\\My Files\\\\OVGU\\\\Hiwi\\\\data mining 2\\\\advanced-data-mining\\\\data\\\\data.csv' Data points from T1 to T11 are the initial points, so we will apply K-Means on this initial set of points full_data = pd.read_csv(file).set_index('T') initial = full_data[0:11].copy() online = full_data[11:].copy() initial .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y T 1 6.0 2.0 2 7.0 3.0 3 6.5 1.0 4 1.0 1.0 5 2.0 2.0 6 3.0 1.0 7 3.0 2.5 8 2.0 8.0 9 2.0 6.0 10 2.5 7.0 11 4.0 7.0 The following points are used as the initial centroids d = {'Centers': ['C1', 'C2', 'C3', 'C4', 'C5'], 'X': [1, 2.5, 2, 4, 6], 'Y': [1, 2, 7, 7, 2]} centroids = pd.DataFrame(d).set_index('Centers') centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 1.0 1 C2 2.5 2 C3 2.0 7 C4 4.0 7 C5 6.0 2 Now we will apply K-Means on the initial data points, and assign every point to its nearest centroid. After K-Means converges, the final centroids and the assignment of each point are plotted below kmeans = KMeans(n_clusters=5, random_state=0, init=centroids).fit(initial[['X', 'Y']]) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] initial['Centroid'] = labels display(initial) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"X\", y=\"Y\", style=\"Centroid\", hue=\"Centroid\", data=initial, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centroid T 1 6.0 2.0 MC5 2 7.0 3.0 MC5 3 6.5 1.0 MC5 4 1.0 1.0 MC1 5 2.0 2.0 MC2 6 3.0 1.0 MC2 7 3.0 2.5 MC2 8 2.0 8.0 MC3 9 2.0 6.0 MC3 10 2.5 7.0 MC3 11 4.0 7.0 MC4 def add_to_mc(microcluster, x, y, t): # Increment LS for X cft_combined.at[microcluster, 'CF1(X)'] += x # Increment SS for X cft_combined.at[microcluster, 'CF2(X)'] += x**2 # Increment LS for Y cft_combined.at[microcluster, 'CF1(Y)'] += y # Increment SS for Yhttp://localhost:8888/notebooks/clustream.ipynb#New-point-at-T12:-(2,-7) cft_combined.at[microcluster, 'CF2(Y)'] += y**2 # Increment LS for T cft_combined.at[microcluster, 'CF1(T)'] += t # Increment SS for T cft_combined.at[microcluster, 'CF2(T)'] += t**2 # Increment N cft_combined.at[microcluster, 'N'] += 1 def create_new_mc(x, y, t, mc_name): d = {'CF1(X)': x, 'CF2(X)': x**2, 'CF1(Y)': y, 'CF2(Y)': y**2, 'CF1(T)': t, 'CF2(T)': t**2, 'N': 1} row = pd.Series(d, name=mc_name) df = cft_combined.append(row) df.fillna(0, inplace=True) return df def delete_oldest_mc(): # idxmin() returns the index of the oldest value of the column oldest_mc = cft_combined['Mean(T)'].idxmin() return cft_combined.drop(oldest_mc) def merge_mc(mc1, mc2, new_mc): feature_vector = ['CF1(X)', 'CF2(X)', 'CF1(Y)', 'CF2(Y)', 'CF1(T)', 'CF2(T)', 'N'] for column in feature_vector: cft_combined.at[new_mc, column] = cft_combined.at[mc1, column] + cft_combined.at[mc2, column] cft_combined.drop([mc1, mc2], inplace=True) cft_combined.fillna(0, inplace=True) return cft_combined def recalculate_summaries(): for index, row in cft_combined.iterrows(): cft_combined.at[index, 'Center(X)'] = row['CF1(X)']/row['N'] cft_combined.at[index, 'Center(Y)'] = row['CF1(Y)']/row['N'] radius_x = math.sqrt( row['CF2(X)']/row['N'] - (row['CF1(X)']/row['N'])**2 ) radius_y = math.sqrt( row['CF2(Y)']/row['N'] - (row['CF1(Y)']/row['N'])**2 ) radius = np.mean([radius_x, radius_y]) cft_combined.at[index, 'Radius(X)'] = radius_x cft_combined.at[index, 'Radius(Y)'] = radius_y cft_combined.at[index, 'Radius'] = radius cft_combined.at[index, 'Mean(T)'] = row['CF1(T)']/row['N'] cft_combined.at[index, 'Sigma(T)'] = math.sqrt( row['CF2(T)']/row['N'] - (row['CF1(T)']/row['N'])**2 ) cft_combined.at[index, 'Max Radius'] = radius * 2 After the initial batch K-Means step, comes the online phase, where new data points arrive one by one. For this, we maintain micro-cluster summaries and update them incrementally as new points arrive. A new point p is handled as follows: 1. Compute the distances between p and each of the q maintained micro-cluster centroids 2. For the closest micro-cluster to p, calculate its max boundary 3. If p is within max boundary, add p to the micro-cluster. 4. If not, delete 1 micro-cluster or merge 2 of the closest located micro-clusters, and create a new micro-cluster with p. First, we initialize the micro-cluster summary structure d = {'CF1(X)': [0.0] * 5, 'CF2(X)': [0.0] * 5, 'CF1(Y)': [0.0] * 5, 'CF2(Y)': [0.0] * 5, 'CF1(T)': [0.0] * 5, 'CF2(T)': [0.0] * 5, 'N': [0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} cft = pd.DataFrame(d).set_index('MicroCluster') for row in initial.itertuples(): c = row.Centroid cft.at[c, 'CF1(X)'] += row.X cft.at[c, 'CF2(X)'] += row.X**2 cft.at[c, 'CF1(Y)'] += row.Y cft.at[c, 'CF2(Y)'] += row.Y**2 cft.at[c, 'CF1(T)'] += row.Index cft.at[c, 'CF2(T)'] += row.Index**2 cft.at[c, 'N'] += 1 # Micro-cluster details d = {'Center(X)': [0.0] * 5, 'Center(Y)': [0.0] * 5, 'Radius(X)': [0.0] * 5, 'Radius(Y)': [0.0] * 5, 'Mean(T)': [0.0] * 5, 'Sigma(T)': [0.0] * 5, 'Radius': [0.0] * 5, 'Max Radius': [0.0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} mc_details = pd.DataFrame(d).set_index('MicroCluster') cft_combined = pd.concat([cft, mc_details], axis=1) recalculate_summaries() display(cft_combined) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745 In the data structures above, the Micro-Cluster summaries are calculated from the initial points. Now, we can use the summary information to handle new data points TIMEPOINT 12 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[12]], s=50) p1.text(2-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC3']['Center(X)'], cft_combined.loc['MC3']['Center(Y)']), 1.052199, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T12: (2, 7) The point falls inside the Max Boundary of the closest Micro-Cluster MC3, so it is absorbed. Since the new point is absorbed by MC3, its feature vector is updated. The feature vectors BEFORE and AFTER adding the new point are displayed display(cft_combined) # Add the new point new_point = list(online.loc[[12]].iloc[0]) add_to_mc('MC3', new_point[0], new_point[1], 12) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC3' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745 #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col0 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col1 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col2 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col3 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col4 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col5 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col6 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col7 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col8 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col9 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col10 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col11 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col12 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col13 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC5 19.5 127.25 6 14 6 14 3 6.5 2 0.408248 0.816497 2 0.816497 0.612372 1.22474 TIMEPOINT 13 timepoint = 13 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 3+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T13: (2.5, 3) The point falls outside the Max Boundary of the closest Micro-Cluster MC2, so a new Micro-Cluster has to be created. In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC5 display(cft_combined) # Add the new point new_point = list(online.loc[[13]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC6') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC6' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.00 0.816497 0.612372 1.224745 #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col0 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col1 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col2 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col3 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col4 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col5 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col6 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col7 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col8 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col9 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col10 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col11 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col12 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col13 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 TIMEPOINT 14 timepoint = 14 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(3.5-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T14: (3.5, 7) The point falls inside the Max Boundary of the closest Micro-Cluster MC4, so it is absorbed. display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC4', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC4' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1.0 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1.0 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col0 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col1 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col2 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col3 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col4 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col5 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col6 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col7 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col8 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col9 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col10 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col11 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col12 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col13 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 TIMEPOINT 15 timepoint = 15 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(7-0.8, 8+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T15: (7, 8) The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created. In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC1 display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC1' else '' for i in x], axis=1)) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC7') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col0 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col1 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col2 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col3 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col4 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col5 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col6 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col7 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col8 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col9 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col10 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col11 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col12 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col13 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 7 49 8 64 15 225 1 7 8 0 0 15 0 0 0 TIMEPOINT 16 timepoint = 16 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(6-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC7']['Center(X)'], cft_combined.loc['MC7']['Center(Y)']), 3.400368, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T16: (6, 7) The point falls inside the Max Boundary of the closest Micro-Cluster MC7, so it is absorbed. NOTE: Max Boundary of a Micro-Cluster with only 1 data point is the distance to the closest Micro-Cluster display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC7', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 7.0 49.00 8.0 64.00 15.0 225.0 1.0 7.000000 8.000000 0.000000 0.000000 15.00 0.000000 0.000000 0.000000 #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 TIMEPOINT 17 timepoint = 17 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 2+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1.09501, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T17: (2.5, 2) The point falls inside the Max Boundary of the closest Micro-Cluster MC2, so it is absorbed. display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC2', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC2' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 13.0 85.00 15.0 113.00 31.0 481.0 2.0 6.500000 7.500000 0.500000 0.500000 15.50 0.500000 0.500000 1.000000 #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col0 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col1 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col2 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col3 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col4 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col5 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col6 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col7 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col8 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col9 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col10 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col11 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col12 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col13 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 TIMEPOINT 18 timepoint = 18 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(5-0.8, 5+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 0.25, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T18: (5, 5) The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created. In order to accommodate the new Micro-Cluster, we first try to delete a Micro-Cluster. Currently, all Micro-Clusters fulfill the relevency threshold, so two of the closest Micro-Clusters should be merged. MC2 and MC6 are currently the closest display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC2' or x.name == 'MC6' else '' for i in x], axis=1)) cft_combined = merge_mc(mc1='MC2', mc2='MC6', new_mc='MC8') recalculate_summaries() # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC9') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC9' else '' for i in x], axis=1) #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col14 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col0 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col1 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col2 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col3 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col4 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col5 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col6 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col7 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col8 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col9 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col10 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col11 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col12 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col13 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 MC8 13 34.5 10.5 24.25 48 568 5 2.6 2.1 0.374166 0.663325 9.6 4.63033 0.518745 1.03749 MC9 5 25 5 25 18 324 1 5 5 0 0 18 0 0 0 We reached the end of the stream. The final Micro-Clusters are plotted below. # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() Now, we run an offline K-Means algorithm with k centroids, in order to get k final clusters. The centers are initialized proportional to the number of points in a given microcluster. We are manually initializing the centroids to the values below: d = {'Centers': ['C1', 'C2', 'C3',], 'X': [3, 3.5, 6], 'Y': [3, 6, 6.5]} weighted_centroids = pd.DataFrame(d).set_index('Centers') weighted_centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 3.0 3.0 C2 3.5 6.0 C3 6.0 6.5 microclusters = cft_combined[['Center(X)', 'Center(Y)']] kmeans = KMeans(n_clusters=3, random_state=0, init=weighted_centroids) kmeans.fit(microclusters) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] microclusters['Centroid'] = labels display(microclusters) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"Centroid\", hue=\"Centroid\", data=microclusters, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Center(X) Center(Y) Centroid MicroCluster MC3 2.125 7.0 MC2 MC4 3.750 7.0 MC2 MC7 6.500 7.5 MC3 MC8 2.600 2.1 MC1 MC9 5.000 5.0 MC2","title":"Clustream"},{"location":"Clustream/clustream/#clustream","text":"import pandas as pd import math import numpy as np from collections import Counter from scipy.spatial import distance from sklearn.cluster import KMeans import seaborn as sns import matplotlib.pyplot as plt sns.set() from IPython.display import display_html def display_side_by_side(*args): html_str='' for df in args: html_str+=df.to_html() display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True) import warnings warnings.filterwarnings('ignore') file = 'C:\\\\My Files\\\\OVGU\\\\Hiwi\\\\data mining 2\\\\advanced-data-mining\\\\data\\\\data.csv'","title":"Clustream"},{"location":"Clustream/clustream/#data-points-from-t1-to-t11-are-the-initial-points-so-we-will-apply-k-means-on-this-initial-set-of-points","text":"full_data = pd.read_csv(file).set_index('T') initial = full_data[0:11].copy() online = full_data[11:].copy() initial .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y T 1 6.0 2.0 2 7.0 3.0 3 6.5 1.0 4 1.0 1.0 5 2.0 2.0 6 3.0 1.0 7 3.0 2.5 8 2.0 8.0 9 2.0 6.0 10 2.5 7.0 11 4.0 7.0","title":"Data points from T1 to T11 are the initial points, so we will apply K-Means on this initial set of points"},{"location":"Clustream/clustream/#the-following-points-are-used-as-the-initial-centroids","text":"d = {'Centers': ['C1', 'C2', 'C3', 'C4', 'C5'], 'X': [1, 2.5, 2, 4, 6], 'Y': [1, 2, 7, 7, 2]} centroids = pd.DataFrame(d).set_index('Centers') centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 1.0 1 C2 2.5 2 C3 2.0 7 C4 4.0 7 C5 6.0 2","title":"The following points are used as the initial centroids"},{"location":"Clustream/clustream/#now-we-will-apply-k-means-on-the-initial-data-points-and-assign-every-point-to-its-nearest-centroid","text":"","title":"Now we will apply K-Means on the initial data points, and assign every point to its nearest centroid."},{"location":"Clustream/clustream/#after-k-means-converges-the-final-centroids-and-the-assignment-of-each-point-are-plotted-below","text":"kmeans = KMeans(n_clusters=5, random_state=0, init=centroids).fit(initial[['X', 'Y']]) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] initial['Centroid'] = labels display(initial) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"X\", y=\"Y\", style=\"Centroid\", hue=\"Centroid\", data=initial, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centroid T 1 6.0 2.0 MC5 2 7.0 3.0 MC5 3 6.5 1.0 MC5 4 1.0 1.0 MC1 5 2.0 2.0 MC2 6 3.0 1.0 MC2 7 3.0 2.5 MC2 8 2.0 8.0 MC3 9 2.0 6.0 MC3 10 2.5 7.0 MC3 11 4.0 7.0 MC4 def add_to_mc(microcluster, x, y, t): # Increment LS for X cft_combined.at[microcluster, 'CF1(X)'] += x # Increment SS for X cft_combined.at[microcluster, 'CF2(X)'] += x**2 # Increment LS for Y cft_combined.at[microcluster, 'CF1(Y)'] += y # Increment SS for Yhttp://localhost:8888/notebooks/clustream.ipynb#New-point-at-T12:-(2,-7) cft_combined.at[microcluster, 'CF2(Y)'] += y**2 # Increment LS for T cft_combined.at[microcluster, 'CF1(T)'] += t # Increment SS for T cft_combined.at[microcluster, 'CF2(T)'] += t**2 # Increment N cft_combined.at[microcluster, 'N'] += 1 def create_new_mc(x, y, t, mc_name): d = {'CF1(X)': x, 'CF2(X)': x**2, 'CF1(Y)': y, 'CF2(Y)': y**2, 'CF1(T)': t, 'CF2(T)': t**2, 'N': 1} row = pd.Series(d, name=mc_name) df = cft_combined.append(row) df.fillna(0, inplace=True) return df def delete_oldest_mc(): # idxmin() returns the index of the oldest value of the column oldest_mc = cft_combined['Mean(T)'].idxmin() return cft_combined.drop(oldest_mc) def merge_mc(mc1, mc2, new_mc): feature_vector = ['CF1(X)', 'CF2(X)', 'CF1(Y)', 'CF2(Y)', 'CF1(T)', 'CF2(T)', 'N'] for column in feature_vector: cft_combined.at[new_mc, column] = cft_combined.at[mc1, column] + cft_combined.at[mc2, column] cft_combined.drop([mc1, mc2], inplace=True) cft_combined.fillna(0, inplace=True) return cft_combined def recalculate_summaries(): for index, row in cft_combined.iterrows(): cft_combined.at[index, 'Center(X)'] = row['CF1(X)']/row['N'] cft_combined.at[index, 'Center(Y)'] = row['CF1(Y)']/row['N'] radius_x = math.sqrt( row['CF2(X)']/row['N'] - (row['CF1(X)']/row['N'])**2 ) radius_y = math.sqrt( row['CF2(Y)']/row['N'] - (row['CF1(Y)']/row['N'])**2 ) radius = np.mean([radius_x, radius_y]) cft_combined.at[index, 'Radius(X)'] = radius_x cft_combined.at[index, 'Radius(Y)'] = radius_y cft_combined.at[index, 'Radius'] = radius cft_combined.at[index, 'Mean(T)'] = row['CF1(T)']/row['N'] cft_combined.at[index, 'Sigma(T)'] = math.sqrt( row['CF2(T)']/row['N'] - (row['CF1(T)']/row['N'])**2 ) cft_combined.at[index, 'Max Radius'] = radius * 2","title":"After K-Means converges, the final centroids and the assignment of each point are plotted below"},{"location":"Clustream/clustream/#after-the-initial-batch-k-means-step-comes-the-online-phase-where-new-data-points-arrive-one-by-one-for-this-we-maintain-micro-cluster-summaries-and-update-them-incrementally-as-new-points-arrive-a-new-point-p-is-handled-as-follows","text":"","title":"After the initial batch K-Means step, comes the online phase, where new data points arrive one by one. For this, we maintain micro-cluster summaries and update them incrementally as new points arrive. A new point p is handled as follows:"},{"location":"Clustream/clustream/#1-compute-the-distances-between-p-and-each-of-the-q-maintained-micro-cluster-centroids","text":"","title":"1. Compute the distances between p and each of the q maintained micro-cluster centroids"},{"location":"Clustream/clustream/#2-for-the-closest-micro-cluster-to-p-calculate-its-max-boundary","text":"","title":"2. For the closest micro-cluster to p, calculate its max boundary"},{"location":"Clustream/clustream/#3-if-p-is-within-max-boundary-add-p-to-the-micro-cluster","text":"","title":"3. If p is within max boundary, add p to the micro-cluster."},{"location":"Clustream/clustream/#4-if-not-delete-1-micro-cluster-or-merge-2-of-the-closest-located-micro-clusters-and-create-a-new-micro-cluster-with-p","text":"","title":"4. If not, delete 1 micro-cluster or merge 2 of the closest located micro-clusters, and create a new micro-cluster with p."},{"location":"Clustream/clustream/#first-we-initialize-the-micro-cluster-summary-structure","text":"d = {'CF1(X)': [0.0] * 5, 'CF2(X)': [0.0] * 5, 'CF1(Y)': [0.0] * 5, 'CF2(Y)': [0.0] * 5, 'CF1(T)': [0.0] * 5, 'CF2(T)': [0.0] * 5, 'N': [0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} cft = pd.DataFrame(d).set_index('MicroCluster') for row in initial.itertuples(): c = row.Centroid cft.at[c, 'CF1(X)'] += row.X cft.at[c, 'CF2(X)'] += row.X**2 cft.at[c, 'CF1(Y)'] += row.Y cft.at[c, 'CF2(Y)'] += row.Y**2 cft.at[c, 'CF1(T)'] += row.Index cft.at[c, 'CF2(T)'] += row.Index**2 cft.at[c, 'N'] += 1 # Micro-cluster details d = {'Center(X)': [0.0] * 5, 'Center(Y)': [0.0] * 5, 'Radius(X)': [0.0] * 5, 'Radius(Y)': [0.0] * 5, 'Mean(T)': [0.0] * 5, 'Sigma(T)': [0.0] * 5, 'Radius': [0.0] * 5, 'Max Radius': [0.0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} mc_details = pd.DataFrame(d).set_index('MicroCluster') cft_combined = pd.concat([cft, mc_details], axis=1) recalculate_summaries() display(cft_combined) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745","title":"First, we initialize the micro-cluster summary structure"},{"location":"Clustream/clustream/#in-the-data-structures-above-the-micro-cluster-summaries-are-calculated-from-the-initial-points-now-we-can-use-the-summary-information-to-handle-new-data-points","text":"","title":"In the data structures above, the Micro-Cluster summaries are calculated from the initial points. Now, we can use the summary information to handle new data points"},{"location":"Clustream/clustream/#timepoint-12","text":"# Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[12]], s=50) p1.text(2-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC3']['Center(X)'], cft_combined.loc['MC3']['Center(Y)']), 1.052199, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 12"},{"location":"Clustream/clustream/#new-point-at-t12-2-7","text":"","title":"New point at T12: (2, 7)"},{"location":"Clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc3-so-it-is-absorbed","text":"","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC3, so it is absorbed."},{"location":"Clustream/clustream/#since-the-new-point-is-absorbed-by-mc3-its-feature-vector-is-updated","text":"","title":"Since the new point is absorbed by MC3, its feature vector  is updated."},{"location":"Clustream/clustream/#the-feature-vectors-before-and-after-adding-the-new-point-are-displayed","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[12]].iloc[0]) add_to_mc('MC3', new_point[0], new_point[1], 12) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC3' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745 #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col0 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col1 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col2 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col3 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col4 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col5 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col6 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col7 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col8 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col9 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col10 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col11 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col12 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col13 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC5 19.5 127.25 6 14 6 14 3 6.5 2 0.408248 0.816497 2 0.816497 0.612372 1.22474","title":"The feature vectors BEFORE and AFTER adding the new point are displayed"},{"location":"Clustream/clustream/#timepoint-13","text":"timepoint = 13 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 3+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 13"},{"location":"Clustream/clustream/#new-point-at-t13-25-3","text":"","title":"New point at T13: (2.5, 3)"},{"location":"Clustream/clustream/#the-point-falls-outside-the-max-boundary-of-the-closest-micro-cluster-mc2-so-a-new-micro-cluster-has-to-be-created","text":"","title":"The point falls outside the Max Boundary of the closest Micro-Cluster MC2, so a new Micro-Cluster has to be created."},{"location":"Clustream/clustream/#in-order-to-accommodate-the-new-micro-cluster-an-old-one-must-be-deleted-the-oldest-one-currently-is-mc5","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[13]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC6') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC6' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.00 0.816497 0.612372 1.224745 #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col0 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col1 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col2 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col3 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col4 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col5 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col6 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col7 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col8 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col9 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col10 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col11 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col12 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col13 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0","title":"In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC5"},{"location":"Clustream/clustream/#timepoint-14","text":"timepoint = 14 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(3.5-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 14"},{"location":"Clustream/clustream/#new-point-at-t14-35-7","text":"","title":"New point at T14: (3.5, 7)"},{"location":"Clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc4-so-it-is-absorbed","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC4', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC4' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1.0 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1.0 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col0 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col1 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col2 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col3 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col4 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col5 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col6 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col7 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col8 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col9 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col10 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col11 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col12 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col13 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC4, so it is absorbed."},{"location":"Clustream/clustream/#timepoint-15","text":"timepoint = 15 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(7-0.8, 8+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 15"},{"location":"Clustream/clustream/#new-point-at-t15-7-8","text":"","title":"New point at T15: (7, 8)"},{"location":"Clustream/clustream/#the-point-falls-outside-the-max-boundary-of-the-closest-micro-cluster-mc4-so-a-new-micro-cluster-has-to-be-created","text":"","title":"The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created."},{"location":"Clustream/clustream/#in-order-to-accommodate-the-new-micro-cluster-an-old-one-must-be-deleted-the-oldest-one-currently-is-mc1","text":"display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC1' else '' for i in x], axis=1)) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC7') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col0 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col1 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col2 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col3 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col4 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col5 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col6 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col7 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col8 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col9 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col10 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col11 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col12 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col13 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 7 49 8 64 15 225 1 7 8 0 0 15 0 0 0","title":"In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC1"},{"location":"Clustream/clustream/#timepoint-16","text":"timepoint = 16 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(6-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC7']['Center(X)'], cft_combined.loc['MC7']['Center(Y)']), 3.400368, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 16"},{"location":"Clustream/clustream/#new-point-at-t16-6-7","text":"","title":"New point at T16: (6, 7)"},{"location":"Clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc7-so-it-is-absorbed","text":"","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC7, so it is absorbed."},{"location":"Clustream/clustream/#note-max-boundary-of-a-micro-cluster-with-only-1-data-point-is-the-distance-to-the-closest-micro-cluster","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC7', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 7.0 49.00 8.0 64.00 15.0 225.0 1.0 7.000000 8.000000 0.000000 0.000000 15.00 0.000000 0.000000 0.000000 #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1","title":"NOTE: Max Boundary of a Micro-Cluster with only 1 data point is the distance to the closest Micro-Cluster"},{"location":"Clustream/clustream/#timepoint-17","text":"timepoint = 17 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 2+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1.09501, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 17"},{"location":"Clustream/clustream/#new-point-at-t17-25-2","text":"","title":"New point at T17: (2.5, 2)"},{"location":"Clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc2-so-it-is-absorbed","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC2', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC2' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 13.0 85.00 15.0 113.00 31.0 481.0 2.0 6.500000 7.500000 0.500000 0.500000 15.50 0.500000 0.500000 1.000000 #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col0 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col1 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col2 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col3 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col4 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col5 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col6 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col7 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col8 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col9 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col10 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col11 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col12 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col13 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC2, so it is absorbed."},{"location":"Clustream/clustream/#timepoint-18","text":"timepoint = 18 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(5-0.8, 5+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 0.25, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 18"},{"location":"Clustream/clustream/#new-point-at-t18-5-5","text":"","title":"New point at T18: (5, 5)"},{"location":"Clustream/clustream/#the-point-falls-outside-the-max-boundary-of-the-closest-micro-cluster-mc4-so-a-new-micro-cluster-has-to-be-created_1","text":"","title":"The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created."},{"location":"Clustream/clustream/#in-order-to-accommodate-the-new-micro-cluster-we-first-try-to-delete-a-micro-cluster-currently-all-micro-clusters-fulfill-the-relevency-threshold-so-two-of-the-closest-micro-clusters-should-be-merged","text":"","title":"In order to accommodate the new Micro-Cluster, we first try to delete a Micro-Cluster. Currently, all Micro-Clusters fulfill the relevency threshold, so two of the closest Micro-Clusters should be merged."},{"location":"Clustream/clustream/#mc2-and-mc6-are-currently-the-closest","text":"display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC2' or x.name == 'MC6' else '' for i in x], axis=1)) cft_combined = merge_mc(mc1='MC2', mc2='MC6', new_mc='MC8') recalculate_summaries() # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC9') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC9' else '' for i in x], axis=1) #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col14 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col0 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col1 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col2 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col3 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col4 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col5 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col6 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col7 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col8 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col9 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col10 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col11 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col12 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col13 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 MC8 13 34.5 10.5 24.25 48 568 5 2.6 2.1 0.374166 0.663325 9.6 4.63033 0.518745 1.03749 MC9 5 25 5 25 18 324 1 5 5 0 0 18 0 0 0","title":"MC2 and MC6 are currently the closest"},{"location":"Clustream/clustream/#we-reached-the-end-of-the-stream-the-final-micro-clusters-are-plotted-below","text":"# Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"We reached the end of the stream. The final Micro-Clusters are plotted below."},{"location":"Clustream/clustream/#now-we-run-an-offline-k-means-algorithm-with-k-centroids-in-order-to-get-k-final-clusters","text":"","title":"Now, we run an offline K-Means algorithm with k centroids, in order to get k final clusters."},{"location":"Clustream/clustream/#the-centers-are-initialized-proportional-to-the-number-of-points-in-a-given-microcluster","text":"","title":"The centers are initialized proportional to the number of points in a given microcluster."},{"location":"Clustream/clustream/#we-are-manually-initializing-the-centroids-to-the-values-below","text":"d = {'Centers': ['C1', 'C2', 'C3',], 'X': [3, 3.5, 6], 'Y': [3, 6, 6.5]} weighted_centroids = pd.DataFrame(d).set_index('Centers') weighted_centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 3.0 3.0 C2 3.5 6.0 C3 6.0 6.5 microclusters = cft_combined[['Center(X)', 'Center(Y)']] kmeans = KMeans(n_clusters=3, random_state=0, init=weighted_centroids) kmeans.fit(microclusters) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] microclusters['Centroid'] = labels display(microclusters) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"Centroid\", hue=\"Centroid\", data=microclusters, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Center(X) Center(Y) Centroid MicroCluster MC3 2.125 7.0 MC2 MC4 3.750 7.0 MC2 MC7 6.500 7.5 MC3 MC8 2.600 2.1 MC1 MC9 5.000 5.0 MC2","title":"We are manually initializing the centroids to the values below:"},{"location":"Fading Function/fading_function/","text":"Fading Function For a damped window model, consider the fading function f(t) = 2^\u2212\u03bbt, where t is the time-point and \u03bb is a user-defined parameter. What is the weight of an instance x observed at time-point T(T > t)? Calculate the weight of the instance x at t0, t1, t2, t3, t4 since time t0. Plot a graph of hte weight v/s the time-point. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() lam = 1 def f(t): return 2**(-lam*t) timepoints = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] weights = [] for timepoint in timepoints: weight = f(timepoint) weights.append(weight) df = pd.DataFrame(weights, columns=[['Weight']]) df['Timepoint'] = df.index df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } Weight Timepoint 0 1.000000 0 1 0.500000 1 2 0.250000 2 3 0.125000 3 4 0.062500 4 5 0.031250 5 6 0.015625 6 7 0.007812 7 8 0.003906 8 9 0.001953 9 10 0.000977 10 fig, ax = plt.subplots() fig.set_size_inches(15, 8.27) plt.title('Weight of an instance X over different timepoints according to the fading function f(t) = 2^\u2212\u03bbt') plt.xlabel('Time') plt.ylabel('Weight') plt.plot(df['Timepoint'].values, df['Weight'].values, marker='o') [<matplotlib.lines.Line2D at 0x138043f0>] As Time increases, the weight of the instance X gets smaller and smaller","title":"Fading Function"},{"location":"Fading Function/fading_function/#fading-function","text":"For a damped window model, consider the fading function f(t) = 2^\u2212\u03bbt, where t is the time-point and \u03bb is a user-defined parameter. What is the weight of an instance x observed at time-point T(T > t)? Calculate the weight of the instance x at t0, t1, t2, t3, t4 since time t0. Plot a graph of hte weight v/s the time-point. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() lam = 1 def f(t): return 2**(-lam*t) timepoints = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] weights = [] for timepoint in timepoints: weight = f(timepoint) weights.append(weight) df = pd.DataFrame(weights, columns=[['Weight']]) df['Timepoint'] = df.index df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } Weight Timepoint 0 1.000000 0 1 0.500000 1 2 0.250000 2 3 0.125000 3 4 0.062500 4 5 0.031250 5 6 0.015625 6 7 0.007812 7 8 0.003906 8 9 0.001953 9 10 0.000977 10 fig, ax = plt.subplots() fig.set_size_inches(15, 8.27) plt.title('Weight of an instance X over different timepoints according to the fading function f(t) = 2^\u2212\u03bbt') plt.xlabel('Time') plt.ylabel('Weight') plt.plot(df['Timepoint'].values, df['Weight'].values, marker='o') [<matplotlib.lines.Line2D at 0x138043f0>]","title":"Fading Function"},{"location":"Fading Function/fading_function/#as-time-increases-the-weight-of-the-instance-x-gets-smaller-and-smaller","text":"","title":"As Time increases, the weight of the instance X gets smaller and smaller"},{"location":"Simple Statistics/simple_statistics/","text":"Simple Statistics Let the following data set be given (sample size 60): 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Calculate: - The mean - The mean recursively - The standard deviation over the sample The calculation of the mean and the standard deviation of a list of numbers is fairly straightforward. import math import time numbers = [4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1] # calculate mean mean = sum(numbers) / len(numbers) # calculate std dev std_dev = math.sqrt( sum([(x - mean)**2 for x in numbers]) / len(numbers) ) print('Sample mean : %0.2f' % mean) print('Sample std dev : %0.2f' % std_dev) Sample mean : 3.87 Sample std dev : 1.77 However, in streaming environments, x is unbounded , which makes it necessary to calculate these simple statistics incrementaly . To incrementally calculate the mean and standard deviation of a random variable x, we need to maintain three variables for x: - LS (Linear Sum) - SS (Squared Sum) - N (Count) This allows observations to be incrementally added. - LS = LS + $x_{i}$ - SS = SS + $x_{i}^2$ - N = N + 1 As shown below, these three variables and their incremental additive properties are sufficient to calculate the mean and standard deviation of x in a streaming environment. class Stream: def __init__(self): self.ls = 0.0 self.ss = 0.0 self.n = 0.0 def increment(self, x): \"\"\" Add x to the observations by incrementing the sufficient stats \"\"\" self.ls += x self.ss += x**2 self.n += 1 def decrement(self, x): \"\"\" Remove x from the observations by decrementing the sufficient stats \"\"\" self.ls -= x self.ss -= x**2 self.n -= 1 def mean(self): \"\"\" Return mean of the observations by dividing LS by N \"\"\" return self.ls/self.n def std_dev(self): \"\"\" Return the standard deviation of the observations \"\"\" return math.sqrt((self.ss/self.n) - (self.ls/self.n)**2) def print_stats(self): \"\"\" Print the current values of the sufficient stats to the console \"\"\" print('Linear Sum : %0.2f' % self.ls) print('Squared Sum : %0.2f' % self.ss) print('N : %0.2f' % self.n) The mean can be calculated by: And the standard deviation can be calculated by: Below, we are incrementally adding three numbers to the sample, and calculating the mean and standard deviation of the observations in the stream stream = Stream() stream.increment(4) stream.increment(3) stream.increment(2) stream.print_stats() print() print('Mean: %0.2f' % stream.mean()) print('Standard Deviation: %0.2f' % stream.std_dev()) Linear Sum : 9.00 Squared Sum : 29.00 N : 3.00 Mean: 3.00 Standard Deviation: 0.82 Coming back to the original sample of 60 items: 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Below, a stream is simulated where the items arrive one by one with some time delay. They are incrementally added to the stream by updating the sufficient statistics, then the sufficient statistics along with the running mean and standard deviation are printed. stream = Stream() for number in numbers: print('Incoming Item: %d' % number) stream.increment(number) print('[LS, SS, N]') print([stream.ls, stream.ss, stream.n]) print() print('Mean: %0.2f, Std Dev: %0.2f' % (stream.mean(), stream.std_dev())) print('=============================') time.sleep(3) Incoming Item: 4 [LS, SS, N] [4.0, 16.0, 1.0] Mean: 4.00, Std Dev: 0.00 ============================= Incoming Item: 3 [LS, SS, N] [7.0, 25.0, 2.0] Mean: 3.50, Std Dev: 0.50 ============================= Incoming Item: 2 [LS, SS, N] [9.0, 29.0, 3.0] Mean: 3.00, Std Dev: 0.82 ============================= Incoming Item: 5 [LS, SS, N] [14.0, 54.0, 4.0] Mean: 3.50, Std Dev: 1.12 ============================= Incoming Item: 4 [LS, SS, N] [18.0, 70.0, 5.0] Mean: 3.60, Std Dev: 1.02 ============================= Incoming Item: 6 [LS, SS, N] [24.0, 106.0, 6.0] Mean: 4.00, Std Dev: 1.29 ============================= Incoming Item: 3 [LS, SS, N] [27.0, 115.0, 7.0] Mean: 3.86, Std Dev: 1.25 ============================= Incoming Item: 7 [LS, SS, N] [34.0, 164.0, 8.0] Mean: 4.25, Std Dev: 1.56 ============================= Incoming Item: 4 [LS, SS, N] [38.0, 180.0, 9.0] Mean: 4.22, Std Dev: 1.47 ============================= Incoming Item: 1 [LS, SS, N] [39.0, 181.0, 10.0] Mean: 3.90, Std Dev: 1.70 ============================= Incoming Item: 4 [LS, SS, N] [43.0, 197.0, 11.0] Mean: 3.91, Std Dev: 1.62 ============================= Incoming Item: 0 [LS, SS, N] [43.0, 197.0, 12.0] Mean: 3.58, Std Dev: 1.89 ============================= Incoming Item: 6 [LS, SS, N] [49.0, 233.0, 13.0] Mean: 3.77, Std Dev: 1.93 ============================= Incoming Item: 4 [LS, SS, N] [53.0, 249.0, 14.0] Mean: 3.79, Std Dev: 1.86 ============================= Incoming Item: 3 [LS, SS, N] [56.0, 258.0, 15.0] Mean: 3.73, Std Dev: 1.81 ============================= Incoming Item: 5 [LS, SS, N] [61.0, 283.0, 16.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [63.0, 287.0, 17.0] Mean: 3.71, Std Dev: 1.77 ============================= Incoming Item: 3 [LS, SS, N] [66.0, 296.0, 18.0] Mean: 3.67, Std Dev: 1.73 ============================= Incoming Item: 5 [LS, SS, N] [71.0, 321.0, 19.0] Mean: 3.74, Std Dev: 1.71 ============================= Incoming Item: 1 [LS, SS, N] [72.0, 322.0, 20.0] Mean: 3.60, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [76.0, 338.0, 21.0] Mean: 3.62, Std Dev: 1.73 ============================= Incoming Item: 4 [LS, SS, N] [80.0, 354.0, 22.0] Mean: 3.64, Std Dev: 1.69 ============================= Incoming Item: 9 [LS, SS, N] [89.0, 435.0, 23.0] Mean: 3.87, Std Dev: 1.98 ============================= Incoming Item: 5 [LS, SS, N] [94.0, 460.0, 24.0] Mean: 3.92, Std Dev: 1.96 ============================= Incoming Item: 4 [LS, SS, N] [98.0, 476.0, 25.0] Mean: 3.92, Std Dev: 1.92 ============================= Incoming Item: 3 [LS, SS, N] [101.0, 485.0, 26.0] Mean: 3.88, Std Dev: 1.89 ============================= Incoming Item: 3 [LS, SS, N] [104.0, 494.0, 27.0] Mean: 3.85, Std Dev: 1.86 ============================= Incoming Item: 5 [LS, SS, N] [109.0, 519.0, 28.0] Mean: 3.89, Std Dev: 1.84 ============================= Incoming Item: 2 [LS, SS, N] [111.0, 523.0, 29.0] Mean: 3.83, Std Dev: 1.84 ============================= Incoming Item: 4 [LS, SS, N] [115.0, 539.0, 30.0] Mean: 3.83, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [118.0, 548.0, 31.0] Mean: 3.81, Std Dev: 1.79 ============================= Incoming Item: 6 [LS, SS, N] [124.0, 584.0, 32.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 5 [LS, SS, N] [129.0, 609.0, 33.0] Mean: 3.91, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [131.0, 613.0, 34.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 6 [LS, SS, N] [137.0, 649.0, 35.0] Mean: 3.91, Std Dev: 1.79 ============================= Incoming Item: 2 [LS, SS, N] [139.0, 653.0, 36.0] Mean: 3.86, Std Dev: 1.80 ============================= Incoming Item: 4 [LS, SS, N] [143.0, 669.0, 37.0] Mean: 3.86, Std Dev: 1.77 ============================= Incoming Item: 5 [LS, SS, N] [148.0, 694.0, 38.0] Mean: 3.89, Std Dev: 1.76 ============================= Incoming Item: 5 [LS, SS, N] [153.0, 719.0, 39.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [154.0, 720.0, 40.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 5 [LS, SS, N] [159.0, 745.0, 41.0] Mean: 3.88, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [163.0, 761.0, 42.0] Mean: 3.88, Std Dev: 1.75 ============================= Incoming Item: 4 [LS, SS, N] [167.0, 777.0, 43.0] Mean: 3.88, Std Dev: 1.73 ============================= Incoming Item: 2 [LS, SS, N] [169.0, 781.0, 44.0] Mean: 3.84, Std Dev: 1.73 ============================= Incoming Item: 7 [LS, SS, N] [176.0, 830.0, 45.0] Mean: 3.91, Std Dev: 1.77 ============================= Incoming Item: 1 [LS, SS, N] [177.0, 831.0, 46.0] Mean: 3.85, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [180.0, 840.0, 47.0] Mean: 3.83, Std Dev: 1.79 ============================= Incoming Item: 3 [LS, SS, N] [183.0, 849.0, 48.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [187.0, 865.0, 49.0] Mean: 3.82, Std Dev: 1.76 ============================= Incoming Item: 7 [LS, SS, N] [194.0, 914.0, 50.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 3 [LS, SS, N] [197.0, 923.0, 51.0] Mean: 3.86, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [201.0, 939.0, 52.0] Mean: 3.87, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [205.0, 955.0, 53.0] Mean: 3.87, Std Dev: 1.75 ============================= Incoming Item: 6 [LS, SS, N] [211.0, 991.0, 54.0] Mean: 3.91, Std Dev: 1.76 ============================= Incoming Item: 6 [LS, SS, N] [217.0, 1027.0, 55.0] Mean: 3.95, Std Dev: 1.76 ============================= Incoming Item: 3 [LS, SS, N] [220.0, 1036.0, 56.0] Mean: 3.93, Std Dev: 1.75 ============================= Incoming Item: 3 [LS, SS, N] [223.0, 1045.0, 57.0] Mean: 3.91, Std Dev: 1.74 ============================= Incoming Item: 2 [LS, SS, N] [225.0, 1049.0, 58.0] Mean: 3.88, Std Dev: 1.74 ============================= Incoming Item: 6 [LS, SS, N] [231.0, 1085.0, 59.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [232.0, 1086.0, 60.0] Mean: 3.87, Std Dev: 1.77 =============================","title":"Simple statistics"},{"location":"Simple Statistics/simple_statistics/#simple-statistics","text":"Let the following data set be given (sample size 60): 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Calculate: - The mean - The mean recursively - The standard deviation over the sample The calculation of the mean and the standard deviation of a list of numbers is fairly straightforward. import math import time numbers = [4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1] # calculate mean mean = sum(numbers) / len(numbers) # calculate std dev std_dev = math.sqrt( sum([(x - mean)**2 for x in numbers]) / len(numbers) ) print('Sample mean : %0.2f' % mean) print('Sample std dev : %0.2f' % std_dev) Sample mean : 3.87 Sample std dev : 1.77 However, in streaming environments, x is unbounded , which makes it necessary to calculate these simple statistics incrementaly . To incrementally calculate the mean and standard deviation of a random variable x, we need to maintain three variables for x: - LS (Linear Sum) - SS (Squared Sum) - N (Count) This allows observations to be incrementally added. - LS = LS + $x_{i}$ - SS = SS + $x_{i}^2$ - N = N + 1 As shown below, these three variables and their incremental additive properties are sufficient to calculate the mean and standard deviation of x in a streaming environment. class Stream: def __init__(self): self.ls = 0.0 self.ss = 0.0 self.n = 0.0 def increment(self, x): \"\"\" Add x to the observations by incrementing the sufficient stats \"\"\" self.ls += x self.ss += x**2 self.n += 1 def decrement(self, x): \"\"\" Remove x from the observations by decrementing the sufficient stats \"\"\" self.ls -= x self.ss -= x**2 self.n -= 1 def mean(self): \"\"\" Return mean of the observations by dividing LS by N \"\"\" return self.ls/self.n def std_dev(self): \"\"\" Return the standard deviation of the observations \"\"\" return math.sqrt((self.ss/self.n) - (self.ls/self.n)**2) def print_stats(self): \"\"\" Print the current values of the sufficient stats to the console \"\"\" print('Linear Sum : %0.2f' % self.ls) print('Squared Sum : %0.2f' % self.ss) print('N : %0.2f' % self.n) The mean can be calculated by: And the standard deviation can be calculated by: Below, we are incrementally adding three numbers to the sample, and calculating the mean and standard deviation of the observations in the stream stream = Stream() stream.increment(4) stream.increment(3) stream.increment(2) stream.print_stats() print() print('Mean: %0.2f' % stream.mean()) print('Standard Deviation: %0.2f' % stream.std_dev()) Linear Sum : 9.00 Squared Sum : 29.00 N : 3.00 Mean: 3.00 Standard Deviation: 0.82 Coming back to the original sample of 60 items: 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Below, a stream is simulated where the items arrive one by one with some time delay. They are incrementally added to the stream by updating the sufficient statistics, then the sufficient statistics along with the running mean and standard deviation are printed. stream = Stream() for number in numbers: print('Incoming Item: %d' % number) stream.increment(number) print('[LS, SS, N]') print([stream.ls, stream.ss, stream.n]) print() print('Mean: %0.2f, Std Dev: %0.2f' % (stream.mean(), stream.std_dev())) print('=============================') time.sleep(3) Incoming Item: 4 [LS, SS, N] [4.0, 16.0, 1.0] Mean: 4.00, Std Dev: 0.00 ============================= Incoming Item: 3 [LS, SS, N] [7.0, 25.0, 2.0] Mean: 3.50, Std Dev: 0.50 ============================= Incoming Item: 2 [LS, SS, N] [9.0, 29.0, 3.0] Mean: 3.00, Std Dev: 0.82 ============================= Incoming Item: 5 [LS, SS, N] [14.0, 54.0, 4.0] Mean: 3.50, Std Dev: 1.12 ============================= Incoming Item: 4 [LS, SS, N] [18.0, 70.0, 5.0] Mean: 3.60, Std Dev: 1.02 ============================= Incoming Item: 6 [LS, SS, N] [24.0, 106.0, 6.0] Mean: 4.00, Std Dev: 1.29 ============================= Incoming Item: 3 [LS, SS, N] [27.0, 115.0, 7.0] Mean: 3.86, Std Dev: 1.25 ============================= Incoming Item: 7 [LS, SS, N] [34.0, 164.0, 8.0] Mean: 4.25, Std Dev: 1.56 ============================= Incoming Item: 4 [LS, SS, N] [38.0, 180.0, 9.0] Mean: 4.22, Std Dev: 1.47 ============================= Incoming Item: 1 [LS, SS, N] [39.0, 181.0, 10.0] Mean: 3.90, Std Dev: 1.70 ============================= Incoming Item: 4 [LS, SS, N] [43.0, 197.0, 11.0] Mean: 3.91, Std Dev: 1.62 ============================= Incoming Item: 0 [LS, SS, N] [43.0, 197.0, 12.0] Mean: 3.58, Std Dev: 1.89 ============================= Incoming Item: 6 [LS, SS, N] [49.0, 233.0, 13.0] Mean: 3.77, Std Dev: 1.93 ============================= Incoming Item: 4 [LS, SS, N] [53.0, 249.0, 14.0] Mean: 3.79, Std Dev: 1.86 ============================= Incoming Item: 3 [LS, SS, N] [56.0, 258.0, 15.0] Mean: 3.73, Std Dev: 1.81 ============================= Incoming Item: 5 [LS, SS, N] [61.0, 283.0, 16.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [63.0, 287.0, 17.0] Mean: 3.71, Std Dev: 1.77 ============================= Incoming Item: 3 [LS, SS, N] [66.0, 296.0, 18.0] Mean: 3.67, Std Dev: 1.73 ============================= Incoming Item: 5 [LS, SS, N] [71.0, 321.0, 19.0] Mean: 3.74, Std Dev: 1.71 ============================= Incoming Item: 1 [LS, SS, N] [72.0, 322.0, 20.0] Mean: 3.60, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [76.0, 338.0, 21.0] Mean: 3.62, Std Dev: 1.73 ============================= Incoming Item: 4 [LS, SS, N] [80.0, 354.0, 22.0] Mean: 3.64, Std Dev: 1.69 ============================= Incoming Item: 9 [LS, SS, N] [89.0, 435.0, 23.0] Mean: 3.87, Std Dev: 1.98 ============================= Incoming Item: 5 [LS, SS, N] [94.0, 460.0, 24.0] Mean: 3.92, Std Dev: 1.96 ============================= Incoming Item: 4 [LS, SS, N] [98.0, 476.0, 25.0] Mean: 3.92, Std Dev: 1.92 ============================= Incoming Item: 3 [LS, SS, N] [101.0, 485.0, 26.0] Mean: 3.88, Std Dev: 1.89 ============================= Incoming Item: 3 [LS, SS, N] [104.0, 494.0, 27.0] Mean: 3.85, Std Dev: 1.86 ============================= Incoming Item: 5 [LS, SS, N] [109.0, 519.0, 28.0] Mean: 3.89, Std Dev: 1.84 ============================= Incoming Item: 2 [LS, SS, N] [111.0, 523.0, 29.0] Mean: 3.83, Std Dev: 1.84 ============================= Incoming Item: 4 [LS, SS, N] [115.0, 539.0, 30.0] Mean: 3.83, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [118.0, 548.0, 31.0] Mean: 3.81, Std Dev: 1.79 ============================= Incoming Item: 6 [LS, SS, N] [124.0, 584.0, 32.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 5 [LS, SS, N] [129.0, 609.0, 33.0] Mean: 3.91, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [131.0, 613.0, 34.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 6 [LS, SS, N] [137.0, 649.0, 35.0] Mean: 3.91, Std Dev: 1.79 ============================= Incoming Item: 2 [LS, SS, N] [139.0, 653.0, 36.0] Mean: 3.86, Std Dev: 1.80 ============================= Incoming Item: 4 [LS, SS, N] [143.0, 669.0, 37.0] Mean: 3.86, Std Dev: 1.77 ============================= Incoming Item: 5 [LS, SS, N] [148.0, 694.0, 38.0] Mean: 3.89, Std Dev: 1.76 ============================= Incoming Item: 5 [LS, SS, N] [153.0, 719.0, 39.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [154.0, 720.0, 40.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 5 [LS, SS, N] [159.0, 745.0, 41.0] Mean: 3.88, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [163.0, 761.0, 42.0] Mean: 3.88, Std Dev: 1.75 ============================= Incoming Item: 4 [LS, SS, N] [167.0, 777.0, 43.0] Mean: 3.88, Std Dev: 1.73 ============================= Incoming Item: 2 [LS, SS, N] [169.0, 781.0, 44.0] Mean: 3.84, Std Dev: 1.73 ============================= Incoming Item: 7 [LS, SS, N] [176.0, 830.0, 45.0] Mean: 3.91, Std Dev: 1.77 ============================= Incoming Item: 1 [LS, SS, N] [177.0, 831.0, 46.0] Mean: 3.85, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [180.0, 840.0, 47.0] Mean: 3.83, Std Dev: 1.79 ============================= Incoming Item: 3 [LS, SS, N] [183.0, 849.0, 48.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [187.0, 865.0, 49.0] Mean: 3.82, Std Dev: 1.76 ============================= Incoming Item: 7 [LS, SS, N] [194.0, 914.0, 50.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 3 [LS, SS, N] [197.0, 923.0, 51.0] Mean: 3.86, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [201.0, 939.0, 52.0] Mean: 3.87, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [205.0, 955.0, 53.0] Mean: 3.87, Std Dev: 1.75 ============================= Incoming Item: 6 [LS, SS, N] [211.0, 991.0, 54.0] Mean: 3.91, Std Dev: 1.76 ============================= Incoming Item: 6 [LS, SS, N] [217.0, 1027.0, 55.0] Mean: 3.95, Std Dev: 1.76 ============================= Incoming Item: 3 [LS, SS, N] [220.0, 1036.0, 56.0] Mean: 3.93, Std Dev: 1.75 ============================= Incoming Item: 3 [LS, SS, N] [223.0, 1045.0, 57.0] Mean: 3.91, Std Dev: 1.74 ============================= Incoming Item: 2 [LS, SS, N] [225.0, 1049.0, 58.0] Mean: 3.88, Std Dev: 1.74 ============================= Incoming Item: 6 [LS, SS, N] [231.0, 1085.0, 59.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [232.0, 1086.0, 60.0] Mean: 3.87, Std Dev: 1.77 =============================","title":"Simple Statistics"}]}