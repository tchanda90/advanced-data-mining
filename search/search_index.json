{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Advanced Data Mining","title":"Advanced Data Mining"},{"location":"#advanced-data-mining","text":"","title":"Advanced Data Mining"},{"location":"change_detection/change_detection/","text":"import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from matplotlib import rcParams rcParams['figure.figsize'] = 11.7,8.27 sns.set() In order to detect change between two windows of a stream S, we test if the P(x) in the current window is different from the P(x) in the previous window Drift has occurred if P(x)ti != P(x)ti+1 To determine if the change in the observed P(x) is the sign of a drift, and that it is not just due to chance, a significance test can be used. Kolmogorov-Smirnov Test Given below are the observed frequencies of grades obtained by a sample of OVGU students in 2018 and 2019. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 19, 13, 12, 7, 9, 3, 12, 3], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 19 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 3 Tirtha believes that the grades of the students have improved from last year (drift). However, Vishnu is skeptical and suspects that the shift in grades is very small and not significant enough to conclude that anything has improved. The Kolmogorov-Smirnov Test can help them determine who is right. KS Test Steps: 1) Calculate the CDFs of both the distributions 2) Find the maximum absolute difference max|D| between the two CDFS 3) Compare max|D| with the critical value at a desired alpha obtained from the KS table. 4) Conclude that the change is significant if max|D| > critical value grades['proportion (2018)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['proportion (2019)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) Grade 1.0 9 4 0.09 0.04 1.3 5 18 0.05 0.18 1.7 12 19 0.12 0.19 2.0 18 13 0.18 0.13 2.3 16 12 0.16 0.12 2.7 12 7 0.12 0.07 3.0 15 9 0.15 0.09 3.3 5 3 0.05 0.03 3.7 2 12 0.02 0.12 4.0 6 3 0.06 0.03 The CDFs and their absolute differences are calculated below grades['cdf (2018)'] = grades['proportion (2018)'].cumsum() grades['cdf (2019)'] = grades['proportion (2019)'].cumsum() grades['D'] = grades.apply(lambda x: np.round(np.abs(x['cdf (2018)'] - x['cdf (2019)']), 2), axis=1) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) cdf (2018) cdf (2019) D Grade 1.0 9 4 0.09 0.04 0.09 0.04 0.05 1.3 5 18 0.05 0.18 0.14 0.22 0.08 1.7 12 19 0.12 0.19 0.26 0.41 0.15 2.0 18 13 0.18 0.13 0.44 0.54 0.10 2.3 16 12 0.16 0.12 0.60 0.66 0.06 2.7 12 7 0.12 0.07 0.72 0.73 0.01 3.0 15 9 0.15 0.09 0.87 0.82 0.05 3.3 5 3 0.05 0.03 0.92 0.85 0.07 3.7 2 12 0.02 0.12 0.94 0.97 0.03 4.0 6 3 0.06 0.03 1.00 1.00 0.00 Below is a visualization of the CDFs and their absolute differences sns.lineplot(data=grades, y=\"cdf (2018)\", x=grades.index) sns.lineplot(data=grades, y=\"cdf (2019)\", x=grades.index) def plot_diff_line(index, row): plt.plot([index, index], [row['cdf (2019)'], row['cdf (2018)']], color='r', linestyle='-', linewidth=2) plt.ylabel(\"Probability\") for index, row in grades.iterrows(): plot_diff_line(index, row) plt.annotate('Max Difference', xy=(1.7, 0.3), xytext=(2, 0.35), arrowprops=dict(facecolor='black', shrink=0.05) ) The Max|D| between the two CDFs is 0.15 From the KS table, the critical value at alpha 0.05 is 1.36/root(n) = 0.136 Since Max|D| > critical value, with 95% confidence, we reject the null hypothesis that the two distributions do not differ, which means we can say that OVGU grades have improved. Tirtha was right. However, Vishnu contests this and says that 95% confidence isn't good enough. He recommends that they be 99% confident before making such a claim about the improvement in grades. So, they look at the KS table again, and they get the critical value at alpha 0.01, which is 1.63/root(n) = 0.163 This time, Max|D| < critical value with 99%; therefore, with 99% confidence, we fail to reject the null hypothesis that the two distributions do not differ, which means that the shift in grades might be due to chance, and the distribution might not have drifted Finding the distance between two probability distributions: Kulback-Leibler Divergence This is a measure to calculate the distance between two probability distributions. Note: this isn't a distance metric because it violates the symmetry and triangle inequality properties of distance metrics. We will use the same grade distributions from earlier. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 19, 13, 12, 7, 9, 3, 12, 3], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 19 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 3 The formula for the KL Divergence is The $ part is a ratio. Therefore, if the two distributions P and Q are almost identical, the probability of x in distribution P will be almost equal to the probability of x in distribution Q, so the ratio will be close to 1. Since the log of a number close to 1 is close to 0, summing multiple numbers close to 0 will result in a low KL Divergence. def kl_divergence(P, Q): kl = 0 for i in range(len(P)): kl += P[i] * np.log(P[i]/Q[i]) return np.round(kl, 3) Steps to calculate KL Divergence for discrete data: 1) Calculate the probabilities for the two distributions from the data 2) Apply the formula # Calculate probability distribution grades['P(x)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['Q(x)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) display(grades) ax = sns.lineplot(data=grades, x=grades.index, y=\"P(x)\", label=\"P(x)\") ax = sns.lineplot(data=grades, x=grades.index, y=\"Q(x)\", label=\"Q(x)\") ax.set(ylabel='Probability', xlabel='Grade') plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 P(x) Q(x) Grade 1.0 9 4 0.09 0.04 1.3 5 18 0.05 0.18 1.7 12 19 0.12 0.19 2.0 18 13 0.18 0.13 2.3 16 12 0.16 0.12 2.7 12 7 0.12 0.07 3.0 15 9 0.15 0.09 3.3 5 3 0.05 0.03 3.7 2 12 0.02 0.12 4.0 6 3 0.06 0.03 px = grades['P(x)'].to_numpy() qx = grades['Q(x)'].to_numpy() print(kl_divergence(px, qx)) print(kl_divergence(qx, px)) 0.242 0.314 The KL Divergence between P and Q is 0.242 If this divergence goes beyond a user-specified threshold, drift is signalled.","title":"Change detection"},{"location":"change_detection/change_detection/#kolmogorov-smirnov-test","text":"Given below are the observed frequencies of grades obtained by a sample of OVGU students in 2018 and 2019. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 19, 13, 12, 7, 9, 3, 12, 3], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 19 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 3 Tirtha believes that the grades of the students have improved from last year (drift). However, Vishnu is skeptical and suspects that the shift in grades is very small and not significant enough to conclude that anything has improved. The Kolmogorov-Smirnov Test can help them determine who is right.","title":"Kolmogorov-Smirnov Test"},{"location":"change_detection/change_detection/#ks-test-steps","text":"1) Calculate the CDFs of both the distributions 2) Find the maximum absolute difference max|D| between the two CDFS 3) Compare max|D| with the critical value at a desired alpha obtained from the KS table. 4) Conclude that the change is significant if max|D| > critical value grades['proportion (2018)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['proportion (2019)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) Grade 1.0 9 4 0.09 0.04 1.3 5 18 0.05 0.18 1.7 12 19 0.12 0.19 2.0 18 13 0.18 0.13 2.3 16 12 0.16 0.12 2.7 12 7 0.12 0.07 3.0 15 9 0.15 0.09 3.3 5 3 0.05 0.03 3.7 2 12 0.02 0.12 4.0 6 3 0.06 0.03","title":"KS Test Steps:"},{"location":"change_detection/change_detection/#the-cdfs-and-their-absolute-differences-are-calculated-below","text":"grades['cdf (2018)'] = grades['proportion (2018)'].cumsum() grades['cdf (2019)'] = grades['proportion (2019)'].cumsum() grades['D'] = grades.apply(lambda x: np.round(np.abs(x['cdf (2018)'] - x['cdf (2019)']), 2), axis=1) grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 proportion (2018) proportion (2019) cdf (2018) cdf (2019) D Grade 1.0 9 4 0.09 0.04 0.09 0.04 0.05 1.3 5 18 0.05 0.18 0.14 0.22 0.08 1.7 12 19 0.12 0.19 0.26 0.41 0.15 2.0 18 13 0.18 0.13 0.44 0.54 0.10 2.3 16 12 0.16 0.12 0.60 0.66 0.06 2.7 12 7 0.12 0.07 0.72 0.73 0.01 3.0 15 9 0.15 0.09 0.87 0.82 0.05 3.3 5 3 0.05 0.03 0.92 0.85 0.07 3.7 2 12 0.02 0.12 0.94 0.97 0.03 4.0 6 3 0.06 0.03 1.00 1.00 0.00","title":"The CDFs and their absolute differences are calculated below"},{"location":"change_detection/change_detection/#below-is-a-visualization-of-the-cdfs-and-their-absolute-differences","text":"sns.lineplot(data=grades, y=\"cdf (2018)\", x=grades.index) sns.lineplot(data=grades, y=\"cdf (2019)\", x=grades.index) def plot_diff_line(index, row): plt.plot([index, index], [row['cdf (2019)'], row['cdf (2018)']], color='r', linestyle='-', linewidth=2) plt.ylabel(\"Probability\") for index, row in grades.iterrows(): plot_diff_line(index, row) plt.annotate('Max Difference', xy=(1.7, 0.3), xytext=(2, 0.35), arrowprops=dict(facecolor='black', shrink=0.05) ) The Max|D| between the two CDFs is 0.15 From the KS table, the critical value at alpha 0.05 is 1.36/root(n) = 0.136 Since Max|D| > critical value, with 95% confidence, we reject the null hypothesis that the two distributions do not differ, which means we can say that OVGU grades have improved. Tirtha was right. However, Vishnu contests this and says that 95% confidence isn't good enough. He recommends that they be 99% confident before making such a claim about the improvement in grades. So, they look at the KS table again, and they get the critical value at alpha 0.01, which is 1.63/root(n) = 0.163 This time, Max|D| < critical value with 99%; therefore, with 99% confidence, we fail to reject the null hypothesis that the two distributions do not differ, which means that the shift in grades might be due to chance, and the distribution might not have drifted","title":"Below is a visualization of the CDFs and their absolute differences"},{"location":"change_detection/change_detection/#finding-the-distance-between-two-probability-distributions-kulback-leibler-divergence","text":"This is a measure to calculate the distance between two probability distributions. Note: this isn't a distance metric because it violates the symmetry and triangle inequality properties of distance metrics. We will use the same grade distributions from earlier. d = {'2018':[9, 5, 12, 18, 16, 12, 15, 5, 2, 6], '2019':[4, 18, 19, 13, 12, 7, 9, 3, 12, 3], 'Grade': [1.0, 1.3, 1.7, 2.0, 2.3, 2.7, 3.0, 3.3, 3.7, 4.0]} grades = pd.DataFrame(d).set_index('Grade') grades .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 Grade 1.0 9 4 1.3 5 18 1.7 12 19 2.0 18 13 2.3 16 12 2.7 12 7 3.0 15 9 3.3 5 3 3.7 2 12 4.0 6 3 The formula for the KL Divergence is The $ part is a ratio. Therefore, if the two distributions P and Q are almost identical, the probability of x in distribution P will be almost equal to the probability of x in distribution Q, so the ratio will be close to 1. Since the log of a number close to 1 is close to 0, summing multiple numbers close to 0 will result in a low KL Divergence. def kl_divergence(P, Q): kl = 0 for i in range(len(P)): kl += P[i] * np.log(P[i]/Q[i]) return np.round(kl, 3)","title":"Finding the distance between two probability distributions: Kulback-Leibler Divergence"},{"location":"change_detection/change_detection/#steps-to-calculate-kl-divergence-for-discrete-data","text":"1) Calculate the probabilities for the two distributions from the data 2) Apply the formula # Calculate probability distribution grades['P(x)'] = grades['2018'].apply(lambda x: x/grades['2018'].sum()) grades['Q(x)'] = grades['2019'].apply(lambda x: x/grades['2019'].sum()) display(grades) ax = sns.lineplot(data=grades, x=grades.index, y=\"P(x)\", label=\"P(x)\") ax = sns.lineplot(data=grades, x=grades.index, y=\"Q(x)\", label=\"Q(x)\") ax.set(ylabel='Probability', xlabel='Grade') plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 2018 2019 P(x) Q(x) Grade 1.0 9 4 0.09 0.04 1.3 5 18 0.05 0.18 1.7 12 19 0.12 0.19 2.0 18 13 0.18 0.13 2.3 16 12 0.16 0.12 2.7 12 7 0.12 0.07 3.0 15 9 0.15 0.09 3.3 5 3 0.05 0.03 3.7 2 12 0.02 0.12 4.0 6 3 0.06 0.03 px = grades['P(x)'].to_numpy() qx = grades['Q(x)'].to_numpy() print(kl_divergence(px, qx)) print(kl_divergence(qx, px)) 0.242 0.314","title":"Steps to calculate KL Divergence for discrete data:"},{"location":"change_detection/change_detection/#the-kl-divergence-between-p-and-q-is-0242","text":"","title":"The KL Divergence between P and Q is 0.242"},{"location":"change_detection/change_detection/#if-this-divergence-goes-beyond-a-user-specified-threshold-drift-is-signalled","text":"","title":"If this divergence goes beyond a user-specified threshold, drift is signalled."},{"location":"clustream/clustream/","text":"Clustream import pandas as pd import math import numpy as np from collections import Counter from scipy.spatial import distance from sklearn.cluster import KMeans import seaborn as sns import matplotlib.pyplot as plt sns.set() from IPython.display import display_html def display_side_by_side(*args): html_str='' for df in args: html_str+=df.to_html() display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True) import warnings warnings.filterwarnings('ignore') file = 'C:\\\\My Files\\\\OVGU\\\\Hiwi\\\\data mining 2\\\\advanced-data-mining\\\\data\\\\data.csv' Data points from T1 to T11 are the initial points, so we will apply K-Means on this initial set of points full_data = pd.read_csv(file).set_index('T') initial = full_data[0:11].copy() online = full_data[11:].copy() initial .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y T 1 6.0 2.0 2 7.0 3.0 3 6.5 1.0 4 1.0 1.0 5 2.0 2.0 6 3.0 1.0 7 3.0 2.5 8 2.0 8.0 9 2.0 6.0 10 2.5 7.0 11 4.0 7.0 The following points are used as the initial centroids d = {'Centers': ['C1', 'C2', 'C3', 'C4', 'C5'], 'X': [1, 2.5, 2, 4, 6], 'Y': [1, 2, 7, 7, 2]} centroids = pd.DataFrame(d).set_index('Centers') centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 1.0 1 C2 2.5 2 C3 2.0 7 C4 4.0 7 C5 6.0 2 Now we will apply K-Means on the initial data points, and assign every point to its nearest centroid. After K-Means converges, the final centroids and the assignment of each point are plotted below kmeans = KMeans(n_clusters=5, random_state=0, init=centroids).fit(initial[['X', 'Y']]) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] initial['Centroid'] = labels display(initial) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"X\", y=\"Y\", style=\"Centroid\", hue=\"Centroid\", data=initial, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centroid T 1 6.0 2.0 MC5 2 7.0 3.0 MC5 3 6.5 1.0 MC5 4 1.0 1.0 MC1 5 2.0 2.0 MC2 6 3.0 1.0 MC2 7 3.0 2.5 MC2 8 2.0 8.0 MC3 9 2.0 6.0 MC3 10 2.5 7.0 MC3 11 4.0 7.0 MC4 def add_to_mc(microcluster, x, y, t): # Increment LS for X cft_combined.at[microcluster, 'CF1(X)'] += x # Increment SS for X cft_combined.at[microcluster, 'CF2(X)'] += x**2 # Increment LS for Y cft_combined.at[microcluster, 'CF1(Y)'] += y # Increment SS for Yhttp://localhost:8888/notebooks/clustream.ipynb#New-point-at-T12:-(2,-7) cft_combined.at[microcluster, 'CF2(Y)'] += y**2 # Increment LS for T cft_combined.at[microcluster, 'CF1(T)'] += t # Increment SS for T cft_combined.at[microcluster, 'CF2(T)'] += t**2 # Increment N cft_combined.at[microcluster, 'N'] += 1 def create_new_mc(x, y, t, mc_name): d = {'CF1(X)': x, 'CF2(X)': x**2, 'CF1(Y)': y, 'CF2(Y)': y**2, 'CF1(T)': t, 'CF2(T)': t**2, 'N': 1} row = pd.Series(d, name=mc_name) df = cft_combined.append(row) df.fillna(0, inplace=True) return df def delete_oldest_mc(): # idxmin() returns the index of the oldest value of the column oldest_mc = cft_combined['Mean(T)'].idxmin() return cft_combined.drop(oldest_mc) def merge_mc(mc1, mc2, new_mc): feature_vector = ['CF1(X)', 'CF2(X)', 'CF1(Y)', 'CF2(Y)', 'CF1(T)', 'CF2(T)', 'N'] for column in feature_vector: cft_combined.at[new_mc, column] = cft_combined.at[mc1, column] + cft_combined.at[mc2, column] cft_combined.drop([mc1, mc2], inplace=True) cft_combined.fillna(0, inplace=True) return cft_combined def recalculate_summaries(): for index, row in cft_combined.iterrows(): cft_combined.at[index, 'Center(X)'] = row['CF1(X)']/row['N'] cft_combined.at[index, 'Center(Y)'] = row['CF1(Y)']/row['N'] radius_x = math.sqrt( row['CF2(X)']/row['N'] - (row['CF1(X)']/row['N'])**2 ) radius_y = math.sqrt( row['CF2(Y)']/row['N'] - (row['CF1(Y)']/row['N'])**2 ) radius = np.mean([radius_x, radius_y]) cft_combined.at[index, 'Radius(X)'] = radius_x cft_combined.at[index, 'Radius(Y)'] = radius_y cft_combined.at[index, 'Radius'] = radius cft_combined.at[index, 'Mean(T)'] = row['CF1(T)']/row['N'] cft_combined.at[index, 'Sigma(T)'] = math.sqrt( row['CF2(T)']/row['N'] - (row['CF1(T)']/row['N'])**2 ) cft_combined.at[index, 'Max Radius'] = radius * 2 After the initial batch K-Means step, comes the online phase, where new data points arrive one by one. For this, we maintain micro-cluster summaries and update them incrementally as new points arrive. A new point p is handled as follows: 1. Compute the distances between p and each of the q maintained micro-cluster centroids 2. For the closest micro-cluster to p, calculate its max boundary 3. If p is within max boundary, add p to the micro-cluster. 4. If not, delete 1 micro-cluster or merge 2 of the closest located micro-clusters, and create a new micro-cluster with p. First, we initialize the micro-cluster summary structure d = {'CF1(X)': [0.0] * 5, 'CF2(X)': [0.0] * 5, 'CF1(Y)': [0.0] * 5, 'CF2(Y)': [0.0] * 5, 'CF1(T)': [0.0] * 5, 'CF2(T)': [0.0] * 5, 'N': [0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} cft = pd.DataFrame(d).set_index('MicroCluster') for row in initial.itertuples(): c = row.Centroid cft.at[c, 'CF1(X)'] += row.X cft.at[c, 'CF2(X)'] += row.X**2 cft.at[c, 'CF1(Y)'] += row.Y cft.at[c, 'CF2(Y)'] += row.Y**2 cft.at[c, 'CF1(T)'] += row.Index cft.at[c, 'CF2(T)'] += row.Index**2 cft.at[c, 'N'] += 1 # Micro-cluster details d = {'Center(X)': [0.0] * 5, 'Center(Y)': [0.0] * 5, 'Radius(X)': [0.0] * 5, 'Radius(Y)': [0.0] * 5, 'Mean(T)': [0.0] * 5, 'Sigma(T)': [0.0] * 5, 'Radius': [0.0] * 5, 'Max Radius': [0.0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} mc_details = pd.DataFrame(d).set_index('MicroCluster') cft_combined = pd.concat([cft, mc_details], axis=1) recalculate_summaries() display(cft_combined) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745 In the data structures above, the Micro-Cluster summaries are calculated from the initial points. Now, we can use the summary information to handle new data points TIMEPOINT 12 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[12]], s=50) p1.text(2-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC3']['Center(X)'], cft_combined.loc['MC3']['Center(Y)']), 1.052199, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T12: (2, 7) The point falls inside the Max Boundary of the closest Micro-Cluster MC3, so it is absorbed. Since the new point is absorbed by MC3, its feature vector is updated. The feature vectors BEFORE and AFTER adding the new point are displayed display(cft_combined) # Add the new point new_point = list(online.loc[[12]].iloc[0]) add_to_mc('MC3', new_point[0], new_point[1], 12) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC3' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745 #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col0 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col1 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col2 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col3 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col4 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col5 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col6 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col7 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col8 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col9 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col10 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col11 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col12 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col13 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC5 19.5 127.25 6 14 6 14 3 6.5 2 0.408248 0.816497 2 0.816497 0.612372 1.22474 TIMEPOINT 13 timepoint = 13 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 3+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T13: (2.5, 3) The point falls outside the Max Boundary of the closest Micro-Cluster MC2, so a new Micro-Cluster has to be created. In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC5 display(cft_combined) # Add the new point new_point = list(online.loc[[13]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC6') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC6' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.00 0.816497 0.612372 1.224745 #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col0 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col1 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col2 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col3 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col4 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col5 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col6 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col7 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col8 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col9 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col10 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col11 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col12 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col13 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 TIMEPOINT 14 timepoint = 14 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(3.5-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T14: (3.5, 7) The point falls inside the Max Boundary of the closest Micro-Cluster MC4, so it is absorbed. display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC4', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC4' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1.0 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1.0 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col0 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col1 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col2 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col3 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col4 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col5 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col6 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col7 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col8 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col9 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col10 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col11 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col12 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col13 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 TIMEPOINT 15 timepoint = 15 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(7-0.8, 8+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T15: (7, 8) The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created. In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC1 display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC1' else '' for i in x], axis=1)) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC7') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col0 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col1 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col2 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col3 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col4 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col5 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col6 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col7 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col8 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col9 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col10 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col11 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col12 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col13 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 7 49 8 64 15 225 1 7 8 0 0 15 0 0 0 TIMEPOINT 16 timepoint = 16 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(6-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC7']['Center(X)'], cft_combined.loc['MC7']['Center(Y)']), 3.400368, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T16: (6, 7) The point falls inside the Max Boundary of the closest Micro-Cluster MC7, so it is absorbed. NOTE: Max Boundary of a Micro-Cluster with only 1 data point is the distance to the closest Micro-Cluster display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC7', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 7.0 49.00 8.0 64.00 15.0 225.0 1.0 7.000000 8.000000 0.000000 0.000000 15.00 0.000000 0.000000 0.000000 #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 TIMEPOINT 17 timepoint = 17 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 2+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1.09501, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T17: (2.5, 2) The point falls inside the Max Boundary of the closest Micro-Cluster MC2, so it is absorbed. display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC2', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC2' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 13.0 85.00 15.0 113.00 31.0 481.0 2.0 6.500000 7.500000 0.500000 0.500000 15.50 0.500000 0.500000 1.000000 #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col0 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col1 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col2 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col3 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col4 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col5 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col6 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col7 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col8 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col9 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col10 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col11 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col12 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col13 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 TIMEPOINT 18 timepoint = 18 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(5-0.8, 5+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 0.25, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() New point at T18: (5, 5) The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created. In order to accommodate the new Micro-Cluster, we first try to delete a Micro-Cluster. Currently, all Micro-Clusters fulfill the relevency threshold, so two of the closest Micro-Clusters should be merged. MC2 and MC6 are currently the closest display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC2' or x.name == 'MC6' else '' for i in x], axis=1)) cft_combined = merge_mc(mc1='MC2', mc2='MC6', new_mc='MC8') recalculate_summaries() # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC9') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC9' else '' for i in x], axis=1) #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col14 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col0 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col1 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col2 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col3 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col4 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col5 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col6 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col7 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col8 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col9 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col10 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col11 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col12 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col13 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 MC8 13 34.5 10.5 24.25 48 568 5 2.6 2.1 0.374166 0.663325 9.6 4.63033 0.518745 1.03749 MC9 5 25 5 25 18 324 1 5 5 0 0 18 0 0 0 We reached the end of the stream. The final Micro-Clusters are plotted below. # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() Now, we run an offline K-Means algorithm with k centroids, in order to get k final clusters. The centers are initialized proportional to the number of points in a given microcluster. We are manually initializing the centroids to the values below: d = {'Centers': ['C1', 'C2', 'C3',], 'X': [3, 3.5, 6], 'Y': [3, 6, 6.5]} weighted_centroids = pd.DataFrame(d).set_index('Centers') weighted_centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 3.0 3.0 C2 3.5 6.0 C3 6.0 6.5 microclusters = cft_combined[['Center(X)', 'Center(Y)']] kmeans = KMeans(n_clusters=3, random_state=0, init=weighted_centroids) kmeans.fit(microclusters) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] microclusters['Centroid'] = labels display(microclusters) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"Centroid\", hue=\"Centroid\", data=microclusters, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Center(X) Center(Y) Centroid MicroCluster MC3 2.125 7.0 MC2 MC4 3.750 7.0 MC2 MC7 6.500 7.5 MC3 MC8 2.600 2.1 MC1 MC9 5.000 5.0 MC2","title":"Clustream"},{"location":"clustream/clustream/#clustream","text":"import pandas as pd import math import numpy as np from collections import Counter from scipy.spatial import distance from sklearn.cluster import KMeans import seaborn as sns import matplotlib.pyplot as plt sns.set() from IPython.display import display_html def display_side_by_side(*args): html_str='' for df in args: html_str+=df.to_html() display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True) import warnings warnings.filterwarnings('ignore') file = 'C:\\\\My Files\\\\OVGU\\\\Hiwi\\\\data mining 2\\\\advanced-data-mining\\\\data\\\\data.csv'","title":"Clustream"},{"location":"clustream/clustream/#data-points-from-t1-to-t11-are-the-initial-points-so-we-will-apply-k-means-on-this-initial-set-of-points","text":"full_data = pd.read_csv(file).set_index('T') initial = full_data[0:11].copy() online = full_data[11:].copy() initial .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y T 1 6.0 2.0 2 7.0 3.0 3 6.5 1.0 4 1.0 1.0 5 2.0 2.0 6 3.0 1.0 7 3.0 2.5 8 2.0 8.0 9 2.0 6.0 10 2.5 7.0 11 4.0 7.0","title":"Data points from T1 to T11 are the initial points, so we will apply K-Means on this initial set of points"},{"location":"clustream/clustream/#the-following-points-are-used-as-the-initial-centroids","text":"d = {'Centers': ['C1', 'C2', 'C3', 'C4', 'C5'], 'X': [1, 2.5, 2, 4, 6], 'Y': [1, 2, 7, 7, 2]} centroids = pd.DataFrame(d).set_index('Centers') centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 1.0 1 C2 2.5 2 C3 2.0 7 C4 4.0 7 C5 6.0 2","title":"The following points are used as the initial centroids"},{"location":"clustream/clustream/#now-we-will-apply-k-means-on-the-initial-data-points-and-assign-every-point-to-its-nearest-centroid","text":"","title":"Now we will apply K-Means on the initial data points, and assign every point to its nearest centroid."},{"location":"clustream/clustream/#after-k-means-converges-the-final-centroids-and-the-assignment-of-each-point-are-plotted-below","text":"kmeans = KMeans(n_clusters=5, random_state=0, init=centroids).fit(initial[['X', 'Y']]) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] initial['Centroid'] = labels display(initial) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"X\", y=\"Y\", style=\"Centroid\", hue=\"Centroid\", data=initial, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centroid T 1 6.0 2.0 MC5 2 7.0 3.0 MC5 3 6.5 1.0 MC5 4 1.0 1.0 MC1 5 2.0 2.0 MC2 6 3.0 1.0 MC2 7 3.0 2.5 MC2 8 2.0 8.0 MC3 9 2.0 6.0 MC3 10 2.5 7.0 MC3 11 4.0 7.0 MC4 def add_to_mc(microcluster, x, y, t): # Increment LS for X cft_combined.at[microcluster, 'CF1(X)'] += x # Increment SS for X cft_combined.at[microcluster, 'CF2(X)'] += x**2 # Increment LS for Y cft_combined.at[microcluster, 'CF1(Y)'] += y # Increment SS for Yhttp://localhost:8888/notebooks/clustream.ipynb#New-point-at-T12:-(2,-7) cft_combined.at[microcluster, 'CF2(Y)'] += y**2 # Increment LS for T cft_combined.at[microcluster, 'CF1(T)'] += t # Increment SS for T cft_combined.at[microcluster, 'CF2(T)'] += t**2 # Increment N cft_combined.at[microcluster, 'N'] += 1 def create_new_mc(x, y, t, mc_name): d = {'CF1(X)': x, 'CF2(X)': x**2, 'CF1(Y)': y, 'CF2(Y)': y**2, 'CF1(T)': t, 'CF2(T)': t**2, 'N': 1} row = pd.Series(d, name=mc_name) df = cft_combined.append(row) df.fillna(0, inplace=True) return df def delete_oldest_mc(): # idxmin() returns the index of the oldest value of the column oldest_mc = cft_combined['Mean(T)'].idxmin() return cft_combined.drop(oldest_mc) def merge_mc(mc1, mc2, new_mc): feature_vector = ['CF1(X)', 'CF2(X)', 'CF1(Y)', 'CF2(Y)', 'CF1(T)', 'CF2(T)', 'N'] for column in feature_vector: cft_combined.at[new_mc, column] = cft_combined.at[mc1, column] + cft_combined.at[mc2, column] cft_combined.drop([mc1, mc2], inplace=True) cft_combined.fillna(0, inplace=True) return cft_combined def recalculate_summaries(): for index, row in cft_combined.iterrows(): cft_combined.at[index, 'Center(X)'] = row['CF1(X)']/row['N'] cft_combined.at[index, 'Center(Y)'] = row['CF1(Y)']/row['N'] radius_x = math.sqrt( row['CF2(X)']/row['N'] - (row['CF1(X)']/row['N'])**2 ) radius_y = math.sqrt( row['CF2(Y)']/row['N'] - (row['CF1(Y)']/row['N'])**2 ) radius = np.mean([radius_x, radius_y]) cft_combined.at[index, 'Radius(X)'] = radius_x cft_combined.at[index, 'Radius(Y)'] = radius_y cft_combined.at[index, 'Radius'] = radius cft_combined.at[index, 'Mean(T)'] = row['CF1(T)']/row['N'] cft_combined.at[index, 'Sigma(T)'] = math.sqrt( row['CF2(T)']/row['N'] - (row['CF1(T)']/row['N'])**2 ) cft_combined.at[index, 'Max Radius'] = radius * 2","title":"After K-Means converges, the final centroids and the assignment of each point are plotted below"},{"location":"clustream/clustream/#after-the-initial-batch-k-means-step-comes-the-online-phase-where-new-data-points-arrive-one-by-one-for-this-we-maintain-micro-cluster-summaries-and-update-them-incrementally-as-new-points-arrive-a-new-point-p-is-handled-as-follows","text":"","title":"After the initial batch K-Means step, comes the online phase, where new data points arrive one by one. For this, we maintain micro-cluster summaries and update them incrementally as new points arrive. A new point p is handled as follows:"},{"location":"clustream/clustream/#1-compute-the-distances-between-p-and-each-of-the-q-maintained-micro-cluster-centroids","text":"","title":"1. Compute the distances between p and each of the q maintained micro-cluster centroids"},{"location":"clustream/clustream/#2-for-the-closest-micro-cluster-to-p-calculate-its-max-boundary","text":"","title":"2. For the closest micro-cluster to p, calculate its max boundary"},{"location":"clustream/clustream/#3-if-p-is-within-max-boundary-add-p-to-the-micro-cluster","text":"","title":"3. If p is within max boundary, add p to the micro-cluster."},{"location":"clustream/clustream/#4-if-not-delete-1-micro-cluster-or-merge-2-of-the-closest-located-micro-clusters-and-create-a-new-micro-cluster-with-p","text":"","title":"4. If not, delete 1 micro-cluster or merge 2 of the closest located micro-clusters, and create a new micro-cluster with p."},{"location":"clustream/clustream/#first-we-initialize-the-micro-cluster-summary-structure","text":"d = {'CF1(X)': [0.0] * 5, 'CF2(X)': [0.0] * 5, 'CF1(Y)': [0.0] * 5, 'CF2(Y)': [0.0] * 5, 'CF1(T)': [0.0] * 5, 'CF2(T)': [0.0] * 5, 'N': [0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} cft = pd.DataFrame(d).set_index('MicroCluster') for row in initial.itertuples(): c = row.Centroid cft.at[c, 'CF1(X)'] += row.X cft.at[c, 'CF2(X)'] += row.X**2 cft.at[c, 'CF1(Y)'] += row.Y cft.at[c, 'CF2(Y)'] += row.Y**2 cft.at[c, 'CF1(T)'] += row.Index cft.at[c, 'CF2(T)'] += row.Index**2 cft.at[c, 'N'] += 1 # Micro-cluster details d = {'Center(X)': [0.0] * 5, 'Center(Y)': [0.0] * 5, 'Radius(X)': [0.0] * 5, 'Radius(Y)': [0.0] * 5, 'Mean(T)': [0.0] * 5, 'Sigma(T)': [0.0] * 5, 'Radius': [0.0] * 5, 'Max Radius': [0.0] * 5, 'MicroCluster': ['MC1', 'MC2', 'MC3', 'MC4', 'MC5']} mc_details = pd.DataFrame(d).set_index('MicroCluster') cft_combined = pd.concat([cft, mc_details], axis=1) recalculate_summaries() display(cft_combined) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745","title":"First, we initialize the micro-cluster summary structure"},{"location":"clustream/clustream/#in-the-data-structures-above-the-micro-cluster-summaries-are-calculated-from-the-initial-points-now-we-can-use-the-summary-information-to-handle-new-data-points","text":"","title":"In the data structures above, the Micro-Cluster summaries are calculated from the initial points. Now, we can use the summary information to handle new data points"},{"location":"clustream/clustream/#timepoint-12","text":"# Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[12]], s=50) p1.text(2-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC3']['Center(X)'], cft_combined.loc['MC3']['Center(Y)']), 1.052199, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 12"},{"location":"clustream/clustream/#new-point-at-t12-2-7","text":"","title":"New point at T12: (2, 7)"},{"location":"clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc3-so-it-is-absorbed","text":"","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC3, so it is absorbed."},{"location":"clustream/clustream/#since-the-new-point-is-absorbed-by-mc3-its-feature-vector-is-updated","text":"","title":"Since the new point is absorbed by MC3, its feature vector  is updated."},{"location":"clustream/clustream/#the-feature-vectors-before-and-after-adding-the-new-point-are-displayed","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[12]].iloc[0]) add_to_mc('MC3', new_point[0], new_point[1], 12) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC3' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.0 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.0 0.816497 0.547507 1.095014 MC3 6.5 14.25 21.0 149.00 27.0 245.0 3 2.166667 7.000000 0.235702 0.816497 9.0 0.816497 0.526099 1.052199 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.0 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.0 0.816497 0.612372 1.224745 #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col0 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col1 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col2 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col3 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col4 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col5 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col6 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col7 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col8 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col9 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col10 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col11 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col12 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col13 { background: lightgreen; } #T_b3f52340_66ce_11e9_9595_c3aaa83f8744row2_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC5 19.5 127.25 6 14 6 14 3 6.5 2 0.408248 0.816497 2 0.816497 0.612372 1.22474","title":"The feature vectors BEFORE and AFTER adding the new point are displayed"},{"location":"clustream/clustream/#timepoint-13","text":"timepoint = 13 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 3+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 13"},{"location":"clustream/clustream/#new-point-at-t13-25-3","text":"","title":"New point at T13: (2.5, 3)"},{"location":"clustream/clustream/#the-point-falls-outside-the-max-boundary-of-the-closest-micro-cluster-mc2-so-a-new-micro-cluster-has-to-be-created","text":"","title":"The point falls outside the Max Boundary of the closest Micro-Cluster MC2, so a new Micro-Cluster has to be created."},{"location":"clustream/clustream/#in-order-to-accommodate-the-new-micro-cluster-an-old-one-must-be-deleted-the-oldest-one-currently-is-mc5","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[13]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC6') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC6' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC5 19.5 127.25 6.0 14.00 6.0 14.0 3 6.500000 2.000000 0.408248 0.816497 2.00 0.816497 0.612372 1.224745 #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col0 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col1 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col2 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col3 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col4 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col5 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col6 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col7 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col8 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col9 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col10 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col11 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col12 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col13 { background: lightgreen; } #T_bb3250cc_66ce_11e9_b8e5_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 4 16 7 49 11 121 1 4 7 0 0 11 0 0 0 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0","title":"In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC5"},{"location":"clustream/clustream/#timepoint-14","text":"timepoint = 14 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(3.5-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 14"},{"location":"clustream/clustream/#new-point-at-t14-35-7","text":"","title":"New point at T14: (3.5, 7)"},{"location":"clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc4-so-it-is-absorbed","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC4', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC4' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1.0 1.00 1.0 1.00 4.0 16.0 1.0 1.000000 1.000000 0.000000 0.000000 4.00 0.000000 0.000000 0.000000 MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 4.0 16.00 7.0 49.00 11.0 121.0 1.0 4.000000 7.000000 0.000000 0.000000 11.00 0.000000 0.000000 0.000000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col0 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col1 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col2 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col3 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col4 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col5 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col6 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col7 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col8 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col9 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col10 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col11 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col12 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col13 { background: lightgreen; } #T_be5e1512_66ce_11e9_86db_c3aaa83f8744row3_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC4, so it is absorbed."},{"location":"clustream/clustream/#timepoint-15","text":"timepoint = 15 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(7-0.8, 8+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 1, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 15"},{"location":"clustream/clustream/#new-point-at-t15-7-8","text":"","title":"New point at T15: (7, 8)"},{"location":"clustream/clustream/#the-point-falls-outside-the-max-boundary-of-the-closest-micro-cluster-mc4-so-a-new-micro-cluster-has-to-be-created","text":"","title":"The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created."},{"location":"clustream/clustream/#in-order-to-accommodate-the-new-micro-cluster-an-old-one-must-be-deleted-the-oldest-one-currently-is-mc1","text":"display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC1' else '' for i in x], axis=1)) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = delete_oldest_mc() cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC7') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col0 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col1 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col2 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col3 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col4 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col5 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col6 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col7 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col8 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col9 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col10 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col11 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col12 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col13 { background: lightcoral; } #T_c48c449e_66ce_11e9_9032_c3aaa83f8744row0_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC1 1 1 1 1 4 16 1 1 1 0 0 4 0 0 0 MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c49d0b1c_66ce_11e9_913c_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 7 49 8 64 15 225 1 7 8 0 0 15 0 0 0","title":"In order to accommodate the new Micro-Cluster, an old one must be deleted. The oldest one currently is MC1"},{"location":"clustream/clustream/#timepoint-16","text":"timepoint = 16 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(6-0.8, 7+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC7']['Center(X)'], cft_combined.loc['MC7']['Center(Y)']), 3.400368, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 16"},{"location":"clustream/clustream/#new-point-at-t16-6-7","text":"","title":"New point at T16: (6, 7)"},{"location":"clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc7-so-it-is-absorbed","text":"","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC7, so it is absorbed."},{"location":"clustream/clustream/#note-max-boundary-of-a-micro-cluster-with-only-1-data-point-is-the-distance-to-the-closest-micro-cluster","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC7', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC7' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 7.0 49.00 8.0 64.00 15.0 225.0 1.0 7.000000 8.000000 0.000000 0.000000 15.00 0.000000 0.000000 0.000000 #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col0 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col1 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col2 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col3 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col4 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col5 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col6 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col7 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col8 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col9 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col10 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col11 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col12 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col13 { background: lightgreen; } #T_c87fe57a_66ce_11e9_aa31_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8 22 5.5 11.25 18 110 3 2.66667 1.83333 0.471405 0.62361 6 0.816497 0.547507 1.09501 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1","title":"NOTE: Max Boundary of a Micro-Cluster with only 1 data point is the distance to the closest Micro-Cluster"},{"location":"clustream/clustream/#timepoint-17","text":"timepoint = 17 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(2.5-0.8, 2+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC2']['Center(X)'], cft_combined.loc['MC2']['Center(Y)']), 1.09501, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 17"},{"location":"clustream/clustream/#new-point-at-t17-25-2","text":"","title":"New point at T17: (2.5, 2)"},{"location":"clustream/clustream/#the-point-falls-inside-the-max-boundary-of-the-closest-micro-cluster-mc2-so-it-is-absorbed","text":"display(cft_combined) # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) add_to_mc('MC2', new_point[0], new_point[1], timepoint) # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC2' else '' for i in x], axis=1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 8.0 22.00 5.5 11.25 18.0 110.0 3.0 2.666667 1.833333 0.471405 0.623610 6.00 0.816497 0.547507 1.095014 MC3 8.5 18.25 28.0 198.00 39.0 389.0 4.0 2.125000 7.000000 0.216506 0.707107 9.75 1.479020 0.461807 0.923613 MC4 7.5 28.25 14.0 98.00 25.0 317.0 2.0 3.750000 7.000000 0.250000 0.000000 12.50 1.500000 0.125000 0.250000 MC6 2.5 6.25 3.0 9.00 13.0 169.0 1.0 2.500000 3.000000 0.000000 0.000000 13.00 0.000000 0.000000 0.000000 MC7 13.0 85.00 15.0 113.00 31.0 481.0 2.0 6.500000 7.500000 0.500000 0.500000 15.50 0.500000 0.500000 1.000000 #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col0 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col1 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col2 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col3 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col4 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col5 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col6 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col7 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col8 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col9 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col10 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col11 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col12 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col13 { background: lightgreen; } #T_c9eb012c_66ce_11e9_925f_c3aaa83f8744row0_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1","title":"The point falls inside the Max Boundary of the closest Micro-Cluster MC2, so it is absorbed."},{"location":"clustream/clustream/#timepoint-18","text":"timepoint = 18 # Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) # New point p1 = sns.scatterplot(x=\"X\", y=\"Y\", data=online.loc[[timepoint]], s=50) p1.text(5-0.8, 5+0.15, \"New Point\", horizontalalignment='left', size='medium', color='black', weight='semibold') # Plot the micro cluster radius ax.add_patch(plt.Circle((cft_combined.loc['MC4']['Center(X)'], cft_combined.loc['MC4']['Center(Y)']), 0.25, color='r', alpha=0.5)) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"TIMEPOINT 18"},{"location":"clustream/clustream/#new-point-at-t18-5-5","text":"","title":"New point at T18: (5, 5)"},{"location":"clustream/clustream/#the-point-falls-outside-the-max-boundary-of-the-closest-micro-cluster-mc4-so-a-new-micro-cluster-has-to-be-created_1","text":"","title":"The point falls outside the Max Boundary of the closest Micro-Cluster MC4, so a new Micro-Cluster has to be created."},{"location":"clustream/clustream/#in-order-to-accommodate-the-new-micro-cluster-we-first-try-to-delete-a-micro-cluster-currently-all-micro-clusters-fulfill-the-relevency-threshold-so-two-of-the-closest-micro-clusters-should-be-merged","text":"","title":"In order to accommodate the new Micro-Cluster, we first try to delete a Micro-Cluster. Currently, all Micro-Clusters fulfill the relevency threshold, so two of the closest Micro-Clusters should be merged."},{"location":"clustream/clustream/#mc2-and-mc6-are-currently-the-closest","text":"display(cft_combined.copy().style.apply(lambda x: ['background: lightcoral' if x.name == 'MC2' or x.name == 'MC6' else '' for i in x], axis=1)) cft_combined = merge_mc(mc1='MC2', mc2='MC6', new_mc='MC8') recalculate_summaries() # Add the new point new_point = list(online.loc[[timepoint]].iloc[0]) cft_combined = create_new_mc(new_point[0], new_point[1], timepoint, 'MC9') # Recalculate cluster summaries recalculate_summaries() cft_combined.copy().style.apply(lambda x: ['background: lightgreen' if x.name == 'MC9' else '' for i in x], axis=1) #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row0_col14 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col0 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col1 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col2 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col3 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col4 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col5 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col6 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col7 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col8 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col9 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col10 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col11 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col12 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col13 { background: lightcoral; } #T_cb9ceb1c_66ce_11e9_8416_c3aaa83f8744row3_col14 { background: lightcoral; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC2 10.5 28.25 7.5 15.25 35 399 4 2.625 1.875 0.414578 0.544862 8.75 4.81534 0.47972 0.95944 MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC6 2.5 6.25 3 9 13 169 1 2.5 3 0 0 13 0 0 0 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col0 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col1 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col2 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col3 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col4 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col5 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col6 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col7 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col8 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col9 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col10 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col11 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col12 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col13 { background: lightgreen; } #T_cbaf11a8_66ce_11e9_bf88_c3aaa83f8744row4_col14 { background: lightgreen; } CF1(X) CF2(X) CF1(Y) CF2(Y) CF1(T) CF2(T) N Center(X) Center(Y) Radius(X) Radius(Y) Mean(T) Sigma(T) Radius Max Radius MicroCluster MC3 8.5 18.25 28 198 39 389 4 2.125 7 0.216506 0.707107 9.75 1.47902 0.461807 0.923613 MC4 7.5 28.25 14 98 25 317 2 3.75 7 0.25 0 12.5 1.5 0.125 0.25 MC7 13 85 15 113 31 481 2 6.5 7.5 0.5 0.5 15.5 0.5 0.5 1 MC8 13 34.5 10.5 24.25 48 568 5 2.6 2.1 0.374166 0.663325 9.6 4.63033 0.518745 1.03749 MC9 5 25 5 25 18 324 1 5 5 0 0 18 0 0 0","title":"MC2 and MC6 are currently the closest"},{"location":"clustream/clustream/#we-reached-the-end-of-the-stream-the-final-micro-clusters-are-plotted-below","text":"# Plot the current micro clusters fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"MicroCluster\", hue=\"MicroCluster\", data=cft_combined.reset_index(), s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show()","title":"We reached the end of the stream. The final Micro-Clusters are plotted below."},{"location":"clustream/clustream/#now-we-run-an-offline-k-means-algorithm-with-k-centroids-in-order-to-get-k-final-clusters","text":"","title":"Now, we run an offline K-Means algorithm with k centroids, in order to get k final clusters."},{"location":"clustream/clustream/#the-centers-are-initialized-proportional-to-the-number-of-points-in-a-given-microcluster","text":"","title":"The centers are initialized proportional to the number of points in a given microcluster."},{"location":"clustream/clustream/#we-are-manually-initializing-the-centroids-to-the-values-below","text":"d = {'Centers': ['C1', 'C2', 'C3',], 'X': [3, 3.5, 6], 'Y': [3, 6, 6.5]} weighted_centroids = pd.DataFrame(d).set_index('Centers') weighted_centroids .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } X Y Centers C1 3.0 3.0 C2 3.5 6.0 C3 6.0 6.5 microclusters = cft_combined[['Center(X)', 'Center(Y)']] kmeans = KMeans(n_clusters=3, random_state=0, init=weighted_centroids) kmeans.fit(microclusters) labels = kmeans.labels_ # Change the cluster labels from just numbers like 0, 1, 2 to MC1, MC2, etc labels = ['MC'+str((x+1)) for x in labels] microclusters['Centroid'] = labels display(microclusters) # Plot the K-means output with assignments fig, ax = plt.subplots() fig.set_size_inches(11.7, 8.27) sns.scatterplot(x=\"Center(X)\", y=\"Center(Y)\", style=\"Centroid\", hue=\"Centroid\", data=microclusters, s=150) #Use adjustable='box-forced' to make the plot area square-shaped as well. ax.set_aspect('equal', adjustable='datalim') ax.plot() #Causes an autoscale update. plt.show() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Center(X) Center(Y) Centroid MicroCluster MC3 2.125 7.0 MC2 MC4 3.750 7.0 MC2 MC7 6.500 7.5 MC3 MC8 2.600 2.1 MC1 MC9 5.000 5.0 MC2","title":"We are manually initializing the centroids to the values below:"},{"location":"fading_function/fading_function/","text":"Fading Function For a damped window model, consider the fading function f(t) = 2^\u2212\u03bbt, where t is the time-point and \u03bb is a user-defined parameter. What is the weight of an instance x observed at time-point T(T > t)? Calculate the weight of the instance x at t0, t1, t2, t3, t4 since time t0. Plot a graph of hte weight v/s the time-point. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() lam = 1 def f(t): return 2**(-lam*t) timepoints = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] weights = [] for timepoint in timepoints: weight = f(timepoint) weights.append(weight) df = pd.DataFrame(weights, columns=[['Weight']]) df['Timepoint'] = df.index df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } Weight Timepoint 0 1.000000 0 1 0.500000 1 2 0.250000 2 3 0.125000 3 4 0.062500 4 5 0.031250 5 6 0.015625 6 7 0.007812 7 8 0.003906 8 9 0.001953 9 10 0.000977 10 fig, ax = plt.subplots() fig.set_size_inches(15, 8.27) plt.title('Weight of an instance X over different timepoints according to the fading function f(t) = 2^\u2212\u03bbt') plt.xlabel('Time') plt.ylabel('Weight') plt.plot(df['Timepoint'].values, df['Weight'].values, marker='o') [<matplotlib.lines.Line2D at 0x138043f0>] As Time increases, the weight of the instance X gets smaller and smaller","title":"Fading Function"},{"location":"fading_function/fading_function/#fading-function","text":"For a damped window model, consider the fading function f(t) = 2^\u2212\u03bbt, where t is the time-point and \u03bb is a user-defined parameter. What is the weight of an instance x observed at time-point T(T > t)? Calculate the weight of the instance x at t0, t1, t2, t3, t4 since time t0. Plot a graph of hte weight v/s the time-point. import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set() lam = 1 def f(t): return 2**(-lam*t) timepoints = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] weights = [] for timepoint in timepoints: weight = f(timepoint) weights.append(weight) df = pd.DataFrame(weights, columns=[['Weight']]) df['Timepoint'] = df.index df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead tr th { text-align: left; } Weight Timepoint 0 1.000000 0 1 0.500000 1 2 0.250000 2 3 0.125000 3 4 0.062500 4 5 0.031250 5 6 0.015625 6 7 0.007812 7 8 0.003906 8 9 0.001953 9 10 0.000977 10 fig, ax = plt.subplots() fig.set_size_inches(15, 8.27) plt.title('Weight of an instance X over different timepoints according to the fading function f(t) = 2^\u2212\u03bbt') plt.xlabel('Time') plt.ylabel('Weight') plt.plot(df['Timepoint'].values, df['Weight'].values, marker='o') [<matplotlib.lines.Line2D at 0x138043f0>]","title":"Fading Function"},{"location":"fading_function/fading_function/#as-time-increases-the-weight-of-the-instance-x-gets-smaller-and-smaller","text":"","title":"As Time increases, the weight of the instance X gets smaller and smaller"},{"location":"incremental_knn/incremental_knn/","text":"import pandas as pd import numpy as np from sklearn.feature_extraction.text import CountVectorizer from sklearn.metrics import accuracy_score, cohen_kappa_score def jaccard_similarity(x, y): \"\"\" Returns jaccard score between x and y \"\"\" return np.logical_and(x, y).sum() / np.logical_or(x, y).sum() d = {'TS': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'Text': ['Regularly paying too much for free trials?', 'Exercise as a chance for your free vehicle.', 'I have just as much fun as I need.', 'Do you like donuts?', 'Fresh donuts available for cheap', 'They had fresh donuts available, so today was fun', 'Register your free trial today', 'What time is good for you?', 'I didn\\'t pay for the donuts', 'Cheap viagra available', 'Did you have a good time today?', 'It was available so I registered'], 'Transformed': ['regular pay free trial', 'exercise chance free vehicle', 'fun need', 'like donut', 'fresh donut available cheap', 'fresh donut available today fun', 'register free trial today', 'time good', 'pay donut', 'cheap viagra available', 'good time today', 'available register'], 'Class': ['Spam', 'Spam', 'Not spam', 'Not spam', 'Spam', 'Not spam', 'Spam', 'Not spam', 'Not spam', 'Spam', 'Not spam', 'Not spam']} data = pd.DataFrame(d) Data stream of documents display(data[['TS', 'Text', 'Class']]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TS Text Class 0 1 Regularly paying too much for free trials? Spam 1 2 Exercise as a chance for your free vehicle. Spam 2 3 I have just as much fun as I need. Not spam 3 4 Do you like donuts? Not spam 4 5 Fresh donuts available for cheap Spam 5 6 They had fresh donuts available, so today was fun Not spam 6 7 Register your free trial today Spam 7 8 What time is good for you? Not spam 8 9 I didn't pay for the donuts Not spam 9 10 Cheap viagra available Spam 10 11 Did you have a good time today? Not spam 11 12 It was available so I registered Not spam Convert the texts into binary vectors where the presence of a term is 1 and the absence is 0. Use the following structure for the document vectors, which excludes stop words: [regular, pay, free, trial, exercise,chance, vehicle, fun, need, like, donut, fresh, available, cheap, register, today, time, good, viagra, run] Note: Assume there is a pre-processing function that stems the terms, so paying becomes pay, trials become trial,etc. vocab = ['regular', 'pay', 'free', 'trial', 'exercise', 'chance', 'vehicle', 'fun', 'need', 'like', 'donut', 'fresh', 'available', 'cheap', 'register', 'today', 'time', 'good', 'viagra', 'run'] vec = CountVectorizer(binary=True, stop_words='english', lowercase=True, vocabulary=vocab) X = vec.fit_transform(data.Transformed) text_vectors = pd.DataFrame(X.toarray(), columns=vec.get_feature_names()) text_vectors['TS'] = data.TS text_vectors.set_index('TS', inplace=True) display(text_vectors) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } regular pay free trial exercise chance vehicle fun need like donut fresh available cheap register today time good viagra run TS 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 6 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 7 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 9 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 10 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 12 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 To measure document similarity, calculate the Jaccard coefficient between two document vectors. Example: doc1=[1, 1, 0, 0] doc2=[1, 0, 1, 0] Jaccard(doc1, doc2) = intersection / union intersection: number of times 1 appears in both docs at the same position (1) union: number of times 1 appears in one vector and either 0 or 1 appears in the other (3) 1 / (1 + 1 + 1) = 0.66 TS 6 Window: [1, 2, 3, 4, 5] print('Jaccard(6, 1):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[1])) print('Jaccard(6, 2):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[2])) print('Jaccard(6, 3):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[3])) print('Jaccard(6, 4):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[4])) print('Jaccard(6, 5):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[5])) Jaccard(6, 1): 0.0 Jaccard(6, 2): 0.0 Jaccard(6, 3): 0.16666666666666666 Jaccard(6, 4): 0.16666666666666666 Jaccard(6, 5): 0.5 Nearest Neighbors: [3: Not spam, 4: Not spam, 5: Spam] Classification: Not spam TS 7 Window: [2, 3, 4, 5, 6] print('Jaccard(7, 2):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[2])) print('Jaccard(7, 3):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[3])) print('Jaccard(7, 4):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[4])) print('Jaccard(7, 5):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[5])) print('Jaccard(7, 6):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[6])) Jaccard(7, 2): 0.14285714285714285 Jaccard(7, 3): 0.0 Jaccard(7, 4): 0.0 Jaccard(7, 5): 0.0 Jaccard(7, 6): 0.125 Nearest Neighbors: [2: Spam, 6: Not spam, 3, 4, 5] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken) Nearest Neighbor: [2: Spam] Classification: Spam TS 8 Window: [3, 4, 5, 6, 7] print('Jaccard(8, 3):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[3])) print('Jaccard(8, 4):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[4])) print('Jaccard(8, 5):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[5])) print('Jaccard(8, 6):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[6])) print('Jaccard(8, 7):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[7])) Jaccard(8, 3): 0.0 Jaccard(8, 4): 0.0 Jaccard(8, 5): 0.0 Jaccard(8, 6): 0.0 Jaccard(8, 7): 0.0 Since all instances are tied, use majority classification Classification: Not spam TS 9 Window: [4, 5, 6, 7, 8] print('Jaccard(9, 4):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[4])) print('Jaccard(9, 5):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[5])) print('Jaccard(9, 6):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[6])) print('Jaccard(9, 7):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[7])) print('Jaccard(9, 8):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[8])) Jaccard(9, 4): 0.3333333333333333 Jaccard(9, 5): 0.2 Jaccard(9, 6): 0.16666666666666666 Jaccard(9, 7): 0.0 Jaccard(9, 8): 0.0 Nearest Neighbors: [4: Not Spam, 6: Not spam, 5, 7, 8] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken) Nearest Neighbor: [4: Not spam, 6: Not spam] Classification: Not spam TS 10 Window: [5, 6, 7, 8, 9] print('Jaccard(10, 5):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[5])) print('Jaccard(10, 6):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[6])) print('Jaccard(10, 7):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[7])) print('Jaccard(10, 8):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[8])) print('Jaccard(10, 9):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[9])) Jaccard(10, 5): 0.4 Jaccard(10, 6): 0.14285714285714285 Jaccard(10, 7): 0.0 Jaccard(10, 8): 0.0 Jaccard(10, 9): 0.0 Nearest Neighbors: [5: Spam, 10: Spam, 7, 8, 9] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken) Nearest Neighbor: [5: Spam, 10: Spam] Classification: Spam TS 11 Window: [6, 7, 8, 9, 10] print('Jaccard(11, 6):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[6])) print('Jaccard(11, 7):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[7])) print('Jaccard(11, 8):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[8])) print('Jaccard(11, 9):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[9])) print('Jaccard(11, 10):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[10])) Jaccard(11, 6): 0.14285714285714285 Jaccard(11, 7): 0.16666666666666666 Jaccard(11, 8): 0.6666666666666666 Jaccard(11, 9): 0.0 Jaccard(11, 10): 0.0 Nearest Neighbors: [8: Not spam, 7: Spam, 6: Not spam] Classification: Not spam TS 12 Window: [7, 8, 9, 10, 11] print('Jaccard(12, 7):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[7])) print('Jaccard(12, 8):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[8])) print('Jaccard(12, 9):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[9])) print('Jaccard(12, 10):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[10])) print('Jaccard(12, 11):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[11])) Jaccard(12, 7): 0.2 Jaccard(12, 8): 0.0 Jaccard(12, 9): 0.0 Jaccard(12, 10): 0.25 Jaccard(12, 11): 0.0 Nearest Neighbors: [10: Spam, 7: Spam, 8, 9, 10] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken) Nearest Neighbor: [10: Spam, 7: Spam] Classification: Spam Summary of predictions r = {'ts': [6, 7, 8, 9, 10, 11, 12], 'pred': ['Not spam', 'Spam', 'Not spam', 'Not spam', 'Spam', 'Not spam', 'Spam'], 'actual': ['Not spam', 'Spam', 'Not spam', 'Not spam', 'Spam', 'Spam', 'Not spam']} results = pd.DataFrame(r).set_index('ts') results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pred actual ts 6 Not spam Not spam 7 Spam Spam 8 Not spam Not spam 9 Not spam Not spam 10 Spam Spam 11 Not spam Spam 12 Spam Not spam When the dataset is imbalanced, computing Kappa against the ground truth gives a more reliable performance estimate than accuracy. Higher values are better print('Accuracy:', np.round(accuracy_score(results.actual, results.pred), 2)) print('Kappa :', np.round(cohen_kappa_score(results.actual, results.pred), 2)) Accuracy: 0.71 Kappa : 0.42","title":"Incremental knn"},{"location":"incremental_knn/incremental_knn/#data-stream-of-documents","text":"display(data[['TS', 'Text', 'Class']]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } TS Text Class 0 1 Regularly paying too much for free trials? Spam 1 2 Exercise as a chance for your free vehicle. Spam 2 3 I have just as much fun as I need. Not spam 3 4 Do you like donuts? Not spam 4 5 Fresh donuts available for cheap Spam 5 6 They had fresh donuts available, so today was fun Not spam 6 7 Register your free trial today Spam 7 8 What time is good for you? Not spam 8 9 I didn't pay for the donuts Not spam 9 10 Cheap viagra available Spam 10 11 Did you have a good time today? Not spam 11 12 It was available so I registered Not spam","title":"Data stream of documents"},{"location":"incremental_knn/incremental_knn/#convert-the-texts-into-binary-vectors-where-the-presence-of-a-term-is-1-and-the-absence-is-0-use-the-following-structure-for-the-document-vectors-which-excludes-stop-words","text":"","title":"Convert  the  texts  into  binary  vectors  where  the  presence  of  a  term  is 1  and  the  absence  is  0.   Use  the  following  structure  for  the  document vectors,  which  excludes  stop  words:"},{"location":"incremental_knn/incremental_knn/#regular-pay-free-trial-exercisechance-vehicle-fun-need-like-donut-fresh-available-cheap-register-today-time-good-viagra-run","text":"","title":"[regular,  pay,  free,  trial,  exercise,chance,  vehicle,  fun,  need,  like,  donut,  fresh,  available,  cheap,  register, today,  time,  good,  viagra,  run]"},{"location":"incremental_knn/incremental_knn/#note-assume-there-is-a-pre-processing-function-that-stems-the-terms-so-paying-becomes-pay-trials-become-trialetc","text":"vocab = ['regular', 'pay', 'free', 'trial', 'exercise', 'chance', 'vehicle', 'fun', 'need', 'like', 'donut', 'fresh', 'available', 'cheap', 'register', 'today', 'time', 'good', 'viagra', 'run'] vec = CountVectorizer(binary=True, stop_words='english', lowercase=True, vocabulary=vocab) X = vec.fit_transform(data.Transformed) text_vectors = pd.DataFrame(X.toarray(), columns=vec.get_feature_names()) text_vectors['TS'] = data.TS text_vectors.set_index('TS', inplace=True) display(text_vectors) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } regular pay free trial exercise chance vehicle fun need like donut fresh available cheap register today time good viagra run TS 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 6 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 7 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 9 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 10 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 11 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 12 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0","title":"Note:  Assume  there  is  a  pre-processing function that stems the terms, so paying becomes pay, trials become trial,etc."},{"location":"incremental_knn/incremental_knn/#to-measure-document-similarity-calculate-the-jaccard-coefficient-between-two-document-vectors","text":"","title":"To measure document similarity, calculate the Jaccard coefficient between two document vectors."},{"location":"incremental_knn/incremental_knn/#example","text":"","title":"Example:"},{"location":"incremental_knn/incremental_knn/#doc11-1-0-0","text":"","title":"doc1=[1, 1, 0, 0]"},{"location":"incremental_knn/incremental_knn/#doc21-0-1-0","text":"","title":"doc2=[1, 0, 1, 0]"},{"location":"incremental_knn/incremental_knn/#jaccarddoc1-doc2-intersection-union","text":"","title":"Jaccard(doc1, doc2) = intersection / union"},{"location":"incremental_knn/incremental_knn/#intersection-number-of-times-1-appears-in-both-docs-at-the-same-position-1","text":"","title":"intersection: number of times 1 appears in both docs at the same position (1)"},{"location":"incremental_knn/incremental_knn/#union-number-of-times-1-appears-in-one-vector-and-either-0-or-1-appears-in-the-other-3","text":"","title":"union: number of times 1 appears in one vector and either 0 or 1 appears in the other (3)"},{"location":"incremental_knn/incremental_knn/#1-1-1-1-066","text":"","title":"1 / (1 + 1 + 1) = 0.66"},{"location":"incremental_knn/incremental_knn/#ts-6","text":"","title":"TS 6"},{"location":"incremental_knn/incremental_knn/#window-1-2-3-4-5","text":"print('Jaccard(6, 1):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[1])) print('Jaccard(6, 2):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[2])) print('Jaccard(6, 3):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[3])) print('Jaccard(6, 4):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[4])) print('Jaccard(6, 5):', jaccard_similarity(text_vectors.loc[6], text_vectors.loc[5])) Jaccard(6, 1): 0.0 Jaccard(6, 2): 0.0 Jaccard(6, 3): 0.16666666666666666 Jaccard(6, 4): 0.16666666666666666 Jaccard(6, 5): 0.5","title":"Window: [1, 2, 3, 4, 5]"},{"location":"incremental_knn/incremental_knn/#nearest-neighbors-3-not-spam-4-not-spam-5-spam","text":"","title":"Nearest Neighbors: [3: Not spam, 4: Not spam, 5: Spam]"},{"location":"incremental_knn/incremental_knn/#classification-not-spam","text":"","title":"Classification: Not spam"},{"location":"incremental_knn/incremental_knn/#ts-7","text":"","title":"TS 7"},{"location":"incremental_knn/incremental_knn/#window-2-3-4-5-6","text":"print('Jaccard(7, 2):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[2])) print('Jaccard(7, 3):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[3])) print('Jaccard(7, 4):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[4])) print('Jaccard(7, 5):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[5])) print('Jaccard(7, 6):', jaccard_similarity(text_vectors.loc[7], text_vectors.loc[6])) Jaccard(7, 2): 0.14285714285714285 Jaccard(7, 3): 0.0 Jaccard(7, 4): 0.0 Jaccard(7, 5): 0.0 Jaccard(7, 6): 0.125","title":"Window: [2, 3, 4, 5, 6]"},{"location":"incremental_knn/incremental_knn/#nearest-neighbors-2-spam-6-not-spam-3-4-5-since-there-are-many-instances-tied-for-3rd-nearest-neighbor-keep-lowering-k-till-the-tie-is-broken","text":"","title":"Nearest Neighbors: [2: Spam, 6: Not spam, 3, 4, 5] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken)"},{"location":"incremental_knn/incremental_knn/#nearest-neighbor-2-spam","text":"","title":"Nearest Neighbor: [2: Spam]"},{"location":"incremental_knn/incremental_knn/#classification-spam","text":"","title":"Classification: Spam"},{"location":"incremental_knn/incremental_knn/#ts-8","text":"","title":"TS 8"},{"location":"incremental_knn/incremental_knn/#window-3-4-5-6-7","text":"print('Jaccard(8, 3):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[3])) print('Jaccard(8, 4):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[4])) print('Jaccard(8, 5):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[5])) print('Jaccard(8, 6):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[6])) print('Jaccard(8, 7):', jaccard_similarity(text_vectors.loc[8], text_vectors.loc[7])) Jaccard(8, 3): 0.0 Jaccard(8, 4): 0.0 Jaccard(8, 5): 0.0 Jaccard(8, 6): 0.0 Jaccard(8, 7): 0.0","title":"Window: [3, 4, 5, 6, 7]"},{"location":"incremental_knn/incremental_knn/#since-all-instances-are-tied-use-majority-classification","text":"","title":"Since all instances are tied, use majority classification"},{"location":"incremental_knn/incremental_knn/#classification-not-spam_1","text":"","title":"Classification: Not spam"},{"location":"incremental_knn/incremental_knn/#ts-9","text":"","title":"TS 9"},{"location":"incremental_knn/incremental_knn/#window-4-5-6-7-8","text":"print('Jaccard(9, 4):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[4])) print('Jaccard(9, 5):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[5])) print('Jaccard(9, 6):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[6])) print('Jaccard(9, 7):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[7])) print('Jaccard(9, 8):', jaccard_similarity(text_vectors.loc[9], text_vectors.loc[8])) Jaccard(9, 4): 0.3333333333333333 Jaccard(9, 5): 0.2 Jaccard(9, 6): 0.16666666666666666 Jaccard(9, 7): 0.0 Jaccard(9, 8): 0.0","title":"Window: [4, 5, 6, 7, 8]"},{"location":"incremental_knn/incremental_knn/#nearest-neighbors-4-not-spam-6-not-spam-5-7-8-since-there-are-many-instances-tied-for-3rd-nearest-neighbor-keep-lowering-k-till-the-tie-is-broken","text":"","title":"Nearest Neighbors: [4: Not Spam, 6: Not spam, 5, 7, 8] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken)"},{"location":"incremental_knn/incremental_knn/#nearest-neighbor-4-not-spam-6-not-spam","text":"","title":"Nearest Neighbor: [4: Not spam, 6: Not spam]"},{"location":"incremental_knn/incremental_knn/#classification-not-spam_2","text":"","title":"Classification: Not spam"},{"location":"incremental_knn/incremental_knn/#ts-10","text":"","title":"TS 10"},{"location":"incremental_knn/incremental_knn/#window-5-6-7-8-9","text":"print('Jaccard(10, 5):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[5])) print('Jaccard(10, 6):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[6])) print('Jaccard(10, 7):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[7])) print('Jaccard(10, 8):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[8])) print('Jaccard(10, 9):', jaccard_similarity(text_vectors.loc[10], text_vectors.loc[9])) Jaccard(10, 5): 0.4 Jaccard(10, 6): 0.14285714285714285 Jaccard(10, 7): 0.0 Jaccard(10, 8): 0.0 Jaccard(10, 9): 0.0","title":"Window: [5, 6, 7, 8, 9]"},{"location":"incremental_knn/incremental_knn/#nearest-neighbors-5-spam-10-spam-7-8-9-since-there-are-many-instances-tied-for-3rd-nearest-neighbor-keep-lowering-k-till-the-tie-is-broken","text":"","title":"Nearest Neighbors: [5: Spam, 10: Spam, 7, 8, 9] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken)"},{"location":"incremental_knn/incremental_knn/#nearest-neighbor-5-spam-10-spam","text":"","title":"Nearest Neighbor: [5: Spam, 10: Spam]"},{"location":"incremental_knn/incremental_knn/#classification-spam_1","text":"","title":"Classification: Spam"},{"location":"incremental_knn/incremental_knn/#ts-11","text":"","title":"TS 11"},{"location":"incremental_knn/incremental_knn/#window-6-7-8-9-10","text":"print('Jaccard(11, 6):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[6])) print('Jaccard(11, 7):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[7])) print('Jaccard(11, 8):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[8])) print('Jaccard(11, 9):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[9])) print('Jaccard(11, 10):', jaccard_similarity(text_vectors.loc[11], text_vectors.loc[10])) Jaccard(11, 6): 0.14285714285714285 Jaccard(11, 7): 0.16666666666666666 Jaccard(11, 8): 0.6666666666666666 Jaccard(11, 9): 0.0 Jaccard(11, 10): 0.0","title":"Window: [6, 7, 8, 9, 10]"},{"location":"incremental_knn/incremental_knn/#nearest-neighbors-8-not-spam-7-spam-6-not-spam","text":"","title":"Nearest Neighbors: [8: Not spam, 7: Spam, 6: Not spam]"},{"location":"incremental_knn/incremental_knn/#classification-not-spam_3","text":"","title":"Classification: Not spam"},{"location":"incremental_knn/incremental_knn/#ts-12","text":"","title":"TS 12"},{"location":"incremental_knn/incremental_knn/#window-7-8-9-10-11","text":"print('Jaccard(12, 7):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[7])) print('Jaccard(12, 8):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[8])) print('Jaccard(12, 9):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[9])) print('Jaccard(12, 10):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[10])) print('Jaccard(12, 11):', jaccard_similarity(text_vectors.loc[12], text_vectors.loc[11])) Jaccard(12, 7): 0.2 Jaccard(12, 8): 0.0 Jaccard(12, 9): 0.0 Jaccard(12, 10): 0.25 Jaccard(12, 11): 0.0","title":"Window: [7, 8, 9, 10, 11]"},{"location":"incremental_knn/incremental_knn/#nearest-neighbors-10-spam-7-spam-8-9-10-since-there-are-many-instances-tied-for-3rd-nearest-neighbor-keep-lowering-k-till-the-tie-is-broken","text":"","title":"Nearest Neighbors: [10: Spam, 7: Spam, 8, 9, 10] (Since there are many instances tied for 3rd nearest neighbor, keep lowering K till the tie is broken)"},{"location":"incremental_knn/incremental_knn/#nearest-neighbor-10-spam-7-spam","text":"","title":"Nearest Neighbor: [10: Spam, 7: Spam]"},{"location":"incremental_knn/incremental_knn/#classification-spam_2","text":"","title":"Classification: Spam"},{"location":"incremental_knn/incremental_knn/#summary-of-predictions","text":"r = {'ts': [6, 7, 8, 9, 10, 11, 12], 'pred': ['Not spam', 'Spam', 'Not spam', 'Not spam', 'Spam', 'Not spam', 'Spam'], 'actual': ['Not spam', 'Spam', 'Not spam', 'Not spam', 'Spam', 'Spam', 'Not spam']} results = pd.DataFrame(r).set_index('ts') results .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pred actual ts 6 Not spam Not spam 7 Spam Spam 8 Not spam Not spam 9 Not spam Not spam 10 Spam Spam 11 Not spam Spam 12 Spam Not spam","title":"Summary of predictions"},{"location":"incremental_knn/incremental_knn/#when-the-dataset-is-imbalanced-computing-kappa-against-the-ground-truth-gives-a-more-reliable-performance-estimate-than-accuracy-higher-values-are-better","text":"print('Accuracy:', np.round(accuracy_score(results.actual, results.pred), 2)) print('Kappa :', np.round(cohen_kappa_score(results.actual, results.pred), 2)) Accuracy: 0.71 Kappa : 0.42","title":"When the dataset is imbalanced, computing Kappa against the ground truth gives a more reliable performance estimate than accuracy. Higher values are better"},{"location":"simple_statistics/simple_statistics/","text":"Simple Statistics Let the following data set be given (sample size 60): 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Calculate: - The mean - The mean recursively - The standard deviation over the sample The calculation of the mean and the standard deviation of a list of numbers is fairly straightforward. import math import time numbers = [4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1] # calculate mean mean = sum(numbers) / len(numbers) # calculate std dev std_dev = math.sqrt( sum([(x - mean)**2 for x in numbers]) / len(numbers) ) print('Sample mean : %0.2f' % mean) print('Sample std dev : %0.2f' % std_dev) Sample mean : 3.87 Sample std dev : 1.77 However, in streaming environments, x is unbounded , which makes it necessary to calculate these simple statistics incrementaly . To incrementally calculate the mean and standard deviation of a random variable x, we need to maintain three variables for x: - LS (Linear Sum) - SS (Squared Sum) - N (Count) This allows observations to be incrementally added. - LS = LS + $x_{i}$ - SS = SS + $x_{i}^2$ - N = N + 1 As shown below, these three variables and their incremental additive properties are sufficient to calculate the mean and standard deviation of x in a streaming environment. class Stream: def __init__(self): self.ls = 0.0 self.ss = 0.0 self.n = 0.0 def increment(self, x): \"\"\" Add x to the observations by incrementing the sufficient stats \"\"\" self.ls += x self.ss += x**2 self.n += 1 def decrement(self, x): \"\"\" Remove x from the observations by decrementing the sufficient stats \"\"\" self.ls -= x self.ss -= x**2 self.n -= 1 def mean(self): \"\"\" Return mean of the observations by dividing LS by N \"\"\" return self.ls/self.n def std_dev(self): \"\"\" Return the standard deviation of the observations \"\"\" return math.sqrt((self.ss/self.n) - (self.ls/self.n)**2) def print_stats(self): \"\"\" Print the current values of the sufficient stats to the console \"\"\" print('Linear Sum : %0.2f' % self.ls) print('Squared Sum : %0.2f' % self.ss) print('N : %0.2f' % self.n) The mean can be calculated by: And the standard deviation can be calculated by: Below, we are incrementally adding three numbers to the sample, and calculating the mean and standard deviation of the observations in the stream stream = Stream() stream.increment(4) stream.increment(3) stream.increment(2) stream.print_stats() print() print('Mean: %0.2f' % stream.mean()) print('Standard Deviation: %0.2f' % stream.std_dev()) Linear Sum : 9.00 Squared Sum : 29.00 N : 3.00 Mean: 3.00 Standard Deviation: 0.82 Coming back to the original sample of 60 items: 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Below, a stream is simulated where the items arrive one by one with some time delay. They are incrementally added to the stream by updating the sufficient statistics, then the sufficient statistics along with the running mean and standard deviation are printed. stream = Stream() for number in numbers: print('Incoming Item: %d' % number) stream.increment(number) print('[LS, SS, N]') print([stream.ls, stream.ss, stream.n]) print() print('Mean: %0.2f, Std Dev: %0.2f' % (stream.mean(), stream.std_dev())) print('=============================') time.sleep(3) Incoming Item: 4 [LS, SS, N] [4.0, 16.0, 1.0] Mean: 4.00, Std Dev: 0.00 ============================= Incoming Item: 3 [LS, SS, N] [7.0, 25.0, 2.0] Mean: 3.50, Std Dev: 0.50 ============================= Incoming Item: 2 [LS, SS, N] [9.0, 29.0, 3.0] Mean: 3.00, Std Dev: 0.82 ============================= Incoming Item: 5 [LS, SS, N] [14.0, 54.0, 4.0] Mean: 3.50, Std Dev: 1.12 ============================= Incoming Item: 4 [LS, SS, N] [18.0, 70.0, 5.0] Mean: 3.60, Std Dev: 1.02 ============================= Incoming Item: 6 [LS, SS, N] [24.0, 106.0, 6.0] Mean: 4.00, Std Dev: 1.29 ============================= Incoming Item: 3 [LS, SS, N] [27.0, 115.0, 7.0] Mean: 3.86, Std Dev: 1.25 ============================= Incoming Item: 7 [LS, SS, N] [34.0, 164.0, 8.0] Mean: 4.25, Std Dev: 1.56 ============================= Incoming Item: 4 [LS, SS, N] [38.0, 180.0, 9.0] Mean: 4.22, Std Dev: 1.47 ============================= Incoming Item: 1 [LS, SS, N] [39.0, 181.0, 10.0] Mean: 3.90, Std Dev: 1.70 ============================= Incoming Item: 4 [LS, SS, N] [43.0, 197.0, 11.0] Mean: 3.91, Std Dev: 1.62 ============================= Incoming Item: 0 [LS, SS, N] [43.0, 197.0, 12.0] Mean: 3.58, Std Dev: 1.89 ============================= Incoming Item: 6 [LS, SS, N] [49.0, 233.0, 13.0] Mean: 3.77, Std Dev: 1.93 ============================= Incoming Item: 4 [LS, SS, N] [53.0, 249.0, 14.0] Mean: 3.79, Std Dev: 1.86 ============================= Incoming Item: 3 [LS, SS, N] [56.0, 258.0, 15.0] Mean: 3.73, Std Dev: 1.81 ============================= Incoming Item: 5 [LS, SS, N] [61.0, 283.0, 16.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [63.0, 287.0, 17.0] Mean: 3.71, Std Dev: 1.77 ============================= Incoming Item: 3 [LS, SS, N] [66.0, 296.0, 18.0] Mean: 3.67, Std Dev: 1.73 ============================= Incoming Item: 5 [LS, SS, N] [71.0, 321.0, 19.0] Mean: 3.74, Std Dev: 1.71 ============================= Incoming Item: 1 [LS, SS, N] [72.0, 322.0, 20.0] Mean: 3.60, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [76.0, 338.0, 21.0] Mean: 3.62, Std Dev: 1.73 ============================= Incoming Item: 4 [LS, SS, N] [80.0, 354.0, 22.0] Mean: 3.64, Std Dev: 1.69 ============================= Incoming Item: 9 [LS, SS, N] [89.0, 435.0, 23.0] Mean: 3.87, Std Dev: 1.98 ============================= Incoming Item: 5 [LS, SS, N] [94.0, 460.0, 24.0] Mean: 3.92, Std Dev: 1.96 ============================= Incoming Item: 4 [LS, SS, N] [98.0, 476.0, 25.0] Mean: 3.92, Std Dev: 1.92 ============================= Incoming Item: 3 [LS, SS, N] [101.0, 485.0, 26.0] Mean: 3.88, Std Dev: 1.89 ============================= Incoming Item: 3 [LS, SS, N] [104.0, 494.0, 27.0] Mean: 3.85, Std Dev: 1.86 ============================= Incoming Item: 5 [LS, SS, N] [109.0, 519.0, 28.0] Mean: 3.89, Std Dev: 1.84 ============================= Incoming Item: 2 [LS, SS, N] [111.0, 523.0, 29.0] Mean: 3.83, Std Dev: 1.84 ============================= Incoming Item: 4 [LS, SS, N] [115.0, 539.0, 30.0] Mean: 3.83, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [118.0, 548.0, 31.0] Mean: 3.81, Std Dev: 1.79 ============================= Incoming Item: 6 [LS, SS, N] [124.0, 584.0, 32.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 5 [LS, SS, N] [129.0, 609.0, 33.0] Mean: 3.91, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [131.0, 613.0, 34.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 6 [LS, SS, N] [137.0, 649.0, 35.0] Mean: 3.91, Std Dev: 1.79 ============================= Incoming Item: 2 [LS, SS, N] [139.0, 653.0, 36.0] Mean: 3.86, Std Dev: 1.80 ============================= Incoming Item: 4 [LS, SS, N] [143.0, 669.0, 37.0] Mean: 3.86, Std Dev: 1.77 ============================= Incoming Item: 5 [LS, SS, N] [148.0, 694.0, 38.0] Mean: 3.89, Std Dev: 1.76 ============================= Incoming Item: 5 [LS, SS, N] [153.0, 719.0, 39.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [154.0, 720.0, 40.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 5 [LS, SS, N] [159.0, 745.0, 41.0] Mean: 3.88, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [163.0, 761.0, 42.0] Mean: 3.88, Std Dev: 1.75 ============================= Incoming Item: 4 [LS, SS, N] [167.0, 777.0, 43.0] Mean: 3.88, Std Dev: 1.73 ============================= Incoming Item: 2 [LS, SS, N] [169.0, 781.0, 44.0] Mean: 3.84, Std Dev: 1.73 ============================= Incoming Item: 7 [LS, SS, N] [176.0, 830.0, 45.0] Mean: 3.91, Std Dev: 1.77 ============================= Incoming Item: 1 [LS, SS, N] [177.0, 831.0, 46.0] Mean: 3.85, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [180.0, 840.0, 47.0] Mean: 3.83, Std Dev: 1.79 ============================= Incoming Item: 3 [LS, SS, N] [183.0, 849.0, 48.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [187.0, 865.0, 49.0] Mean: 3.82, Std Dev: 1.76 ============================= Incoming Item: 7 [LS, SS, N] [194.0, 914.0, 50.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 3 [LS, SS, N] [197.0, 923.0, 51.0] Mean: 3.86, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [201.0, 939.0, 52.0] Mean: 3.87, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [205.0, 955.0, 53.0] Mean: 3.87, Std Dev: 1.75 ============================= Incoming Item: 6 [LS, SS, N] [211.0, 991.0, 54.0] Mean: 3.91, Std Dev: 1.76 ============================= Incoming Item: 6 [LS, SS, N] [217.0, 1027.0, 55.0] Mean: 3.95, Std Dev: 1.76 ============================= Incoming Item: 3 [LS, SS, N] [220.0, 1036.0, 56.0] Mean: 3.93, Std Dev: 1.75 ============================= Incoming Item: 3 [LS, SS, N] [223.0, 1045.0, 57.0] Mean: 3.91, Std Dev: 1.74 ============================= Incoming Item: 2 [LS, SS, N] [225.0, 1049.0, 58.0] Mean: 3.88, Std Dev: 1.74 ============================= Incoming Item: 6 [LS, SS, N] [231.0, 1085.0, 59.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [232.0, 1086.0, 60.0] Mean: 3.87, Std Dev: 1.77 =============================","title":"Simple statistics"},{"location":"simple_statistics/simple_statistics/#simple-statistics","text":"Let the following data set be given (sample size 60): 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Calculate: - The mean - The mean recursively - The standard deviation over the sample The calculation of the mean and the standard deviation of a list of numbers is fairly straightforward. import math import time numbers = [4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1] # calculate mean mean = sum(numbers) / len(numbers) # calculate std dev std_dev = math.sqrt( sum([(x - mean)**2 for x in numbers]) / len(numbers) ) print('Sample mean : %0.2f' % mean) print('Sample std dev : %0.2f' % std_dev) Sample mean : 3.87 Sample std dev : 1.77 However, in streaming environments, x is unbounded , which makes it necessary to calculate these simple statistics incrementaly . To incrementally calculate the mean and standard deviation of a random variable x, we need to maintain three variables for x: - LS (Linear Sum) - SS (Squared Sum) - N (Count) This allows observations to be incrementally added. - LS = LS + $x_{i}$ - SS = SS + $x_{i}^2$ - N = N + 1 As shown below, these three variables and their incremental additive properties are sufficient to calculate the mean and standard deviation of x in a streaming environment. class Stream: def __init__(self): self.ls = 0.0 self.ss = 0.0 self.n = 0.0 def increment(self, x): \"\"\" Add x to the observations by incrementing the sufficient stats \"\"\" self.ls += x self.ss += x**2 self.n += 1 def decrement(self, x): \"\"\" Remove x from the observations by decrementing the sufficient stats \"\"\" self.ls -= x self.ss -= x**2 self.n -= 1 def mean(self): \"\"\" Return mean of the observations by dividing LS by N \"\"\" return self.ls/self.n def std_dev(self): \"\"\" Return the standard deviation of the observations \"\"\" return math.sqrt((self.ss/self.n) - (self.ls/self.n)**2) def print_stats(self): \"\"\" Print the current values of the sufficient stats to the console \"\"\" print('Linear Sum : %0.2f' % self.ls) print('Squared Sum : %0.2f' % self.ss) print('N : %0.2f' % self.n) The mean can be calculated by: And the standard deviation can be calculated by: Below, we are incrementally adding three numbers to the sample, and calculating the mean and standard deviation of the observations in the stream stream = Stream() stream.increment(4) stream.increment(3) stream.increment(2) stream.print_stats() print() print('Mean: %0.2f' % stream.mean()) print('Standard Deviation: %0.2f' % stream.std_dev()) Linear Sum : 9.00 Squared Sum : 29.00 N : 3.00 Mean: 3.00 Standard Deviation: 0.82 Coming back to the original sample of 60 items: 4, 3, 2, 5, 4, 6, 3, 7, 4, 1, 4, 0, 6, 4, 3, 5, 2, 3, 5, 1, 4, 4, 9, 5, 4, 3, 3, 5, 2, 4, 3, 6, 5, 2, 6, 2, 4, 5, 5, 1, 5, 4, 4, 2, 7, 1, 3, 3, 4, 7, 3, 4, 4, 6, 6, 3, 3, 2, 6, 1. Below, a stream is simulated where the items arrive one by one with some time delay. They are incrementally added to the stream by updating the sufficient statistics, then the sufficient statistics along with the running mean and standard deviation are printed. stream = Stream() for number in numbers: print('Incoming Item: %d' % number) stream.increment(number) print('[LS, SS, N]') print([stream.ls, stream.ss, stream.n]) print() print('Mean: %0.2f, Std Dev: %0.2f' % (stream.mean(), stream.std_dev())) print('=============================') time.sleep(3) Incoming Item: 4 [LS, SS, N] [4.0, 16.0, 1.0] Mean: 4.00, Std Dev: 0.00 ============================= Incoming Item: 3 [LS, SS, N] [7.0, 25.0, 2.0] Mean: 3.50, Std Dev: 0.50 ============================= Incoming Item: 2 [LS, SS, N] [9.0, 29.0, 3.0] Mean: 3.00, Std Dev: 0.82 ============================= Incoming Item: 5 [LS, SS, N] [14.0, 54.0, 4.0] Mean: 3.50, Std Dev: 1.12 ============================= Incoming Item: 4 [LS, SS, N] [18.0, 70.0, 5.0] Mean: 3.60, Std Dev: 1.02 ============================= Incoming Item: 6 [LS, SS, N] [24.0, 106.0, 6.0] Mean: 4.00, Std Dev: 1.29 ============================= Incoming Item: 3 [LS, SS, N] [27.0, 115.0, 7.0] Mean: 3.86, Std Dev: 1.25 ============================= Incoming Item: 7 [LS, SS, N] [34.0, 164.0, 8.0] Mean: 4.25, Std Dev: 1.56 ============================= Incoming Item: 4 [LS, SS, N] [38.0, 180.0, 9.0] Mean: 4.22, Std Dev: 1.47 ============================= Incoming Item: 1 [LS, SS, N] [39.0, 181.0, 10.0] Mean: 3.90, Std Dev: 1.70 ============================= Incoming Item: 4 [LS, SS, N] [43.0, 197.0, 11.0] Mean: 3.91, Std Dev: 1.62 ============================= Incoming Item: 0 [LS, SS, N] [43.0, 197.0, 12.0] Mean: 3.58, Std Dev: 1.89 ============================= Incoming Item: 6 [LS, SS, N] [49.0, 233.0, 13.0] Mean: 3.77, Std Dev: 1.93 ============================= Incoming Item: 4 [LS, SS, N] [53.0, 249.0, 14.0] Mean: 3.79, Std Dev: 1.86 ============================= Incoming Item: 3 [LS, SS, N] [56.0, 258.0, 15.0] Mean: 3.73, Std Dev: 1.81 ============================= Incoming Item: 5 [LS, SS, N] [61.0, 283.0, 16.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [63.0, 287.0, 17.0] Mean: 3.71, Std Dev: 1.77 ============================= Incoming Item: 3 [LS, SS, N] [66.0, 296.0, 18.0] Mean: 3.67, Std Dev: 1.73 ============================= Incoming Item: 5 [LS, SS, N] [71.0, 321.0, 19.0] Mean: 3.74, Std Dev: 1.71 ============================= Incoming Item: 1 [LS, SS, N] [72.0, 322.0, 20.0] Mean: 3.60, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [76.0, 338.0, 21.0] Mean: 3.62, Std Dev: 1.73 ============================= Incoming Item: 4 [LS, SS, N] [80.0, 354.0, 22.0] Mean: 3.64, Std Dev: 1.69 ============================= Incoming Item: 9 [LS, SS, N] [89.0, 435.0, 23.0] Mean: 3.87, Std Dev: 1.98 ============================= Incoming Item: 5 [LS, SS, N] [94.0, 460.0, 24.0] Mean: 3.92, Std Dev: 1.96 ============================= Incoming Item: 4 [LS, SS, N] [98.0, 476.0, 25.0] Mean: 3.92, Std Dev: 1.92 ============================= Incoming Item: 3 [LS, SS, N] [101.0, 485.0, 26.0] Mean: 3.88, Std Dev: 1.89 ============================= Incoming Item: 3 [LS, SS, N] [104.0, 494.0, 27.0] Mean: 3.85, Std Dev: 1.86 ============================= Incoming Item: 5 [LS, SS, N] [109.0, 519.0, 28.0] Mean: 3.89, Std Dev: 1.84 ============================= Incoming Item: 2 [LS, SS, N] [111.0, 523.0, 29.0] Mean: 3.83, Std Dev: 1.84 ============================= Incoming Item: 4 [LS, SS, N] [115.0, 539.0, 30.0] Mean: 3.83, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [118.0, 548.0, 31.0] Mean: 3.81, Std Dev: 1.79 ============================= Incoming Item: 6 [LS, SS, N] [124.0, 584.0, 32.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 5 [LS, SS, N] [129.0, 609.0, 33.0] Mean: 3.91, Std Dev: 1.78 ============================= Incoming Item: 2 [LS, SS, N] [131.0, 613.0, 34.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 6 [LS, SS, N] [137.0, 649.0, 35.0] Mean: 3.91, Std Dev: 1.79 ============================= Incoming Item: 2 [LS, SS, N] [139.0, 653.0, 36.0] Mean: 3.86, Std Dev: 1.80 ============================= Incoming Item: 4 [LS, SS, N] [143.0, 669.0, 37.0] Mean: 3.86, Std Dev: 1.77 ============================= Incoming Item: 5 [LS, SS, N] [148.0, 694.0, 38.0] Mean: 3.89, Std Dev: 1.76 ============================= Incoming Item: 5 [LS, SS, N] [153.0, 719.0, 39.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [154.0, 720.0, 40.0] Mean: 3.85, Std Dev: 1.78 ============================= Incoming Item: 5 [LS, SS, N] [159.0, 745.0, 41.0] Mean: 3.88, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [163.0, 761.0, 42.0] Mean: 3.88, Std Dev: 1.75 ============================= Incoming Item: 4 [LS, SS, N] [167.0, 777.0, 43.0] Mean: 3.88, Std Dev: 1.73 ============================= Incoming Item: 2 [LS, SS, N] [169.0, 781.0, 44.0] Mean: 3.84, Std Dev: 1.73 ============================= Incoming Item: 7 [LS, SS, N] [176.0, 830.0, 45.0] Mean: 3.91, Std Dev: 1.77 ============================= Incoming Item: 1 [LS, SS, N] [177.0, 831.0, 46.0] Mean: 3.85, Std Dev: 1.81 ============================= Incoming Item: 3 [LS, SS, N] [180.0, 840.0, 47.0] Mean: 3.83, Std Dev: 1.79 ============================= Incoming Item: 3 [LS, SS, N] [183.0, 849.0, 48.0] Mean: 3.81, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [187.0, 865.0, 49.0] Mean: 3.82, Std Dev: 1.76 ============================= Incoming Item: 7 [LS, SS, N] [194.0, 914.0, 50.0] Mean: 3.88, Std Dev: 1.80 ============================= Incoming Item: 3 [LS, SS, N] [197.0, 923.0, 51.0] Mean: 3.86, Std Dev: 1.78 ============================= Incoming Item: 4 [LS, SS, N] [201.0, 939.0, 52.0] Mean: 3.87, Std Dev: 1.77 ============================= Incoming Item: 4 [LS, SS, N] [205.0, 955.0, 53.0] Mean: 3.87, Std Dev: 1.75 ============================= Incoming Item: 6 [LS, SS, N] [211.0, 991.0, 54.0] Mean: 3.91, Std Dev: 1.76 ============================= Incoming Item: 6 [LS, SS, N] [217.0, 1027.0, 55.0] Mean: 3.95, Std Dev: 1.76 ============================= Incoming Item: 3 [LS, SS, N] [220.0, 1036.0, 56.0] Mean: 3.93, Std Dev: 1.75 ============================= Incoming Item: 3 [LS, SS, N] [223.0, 1045.0, 57.0] Mean: 3.91, Std Dev: 1.74 ============================= Incoming Item: 2 [LS, SS, N] [225.0, 1049.0, 58.0] Mean: 3.88, Std Dev: 1.74 ============================= Incoming Item: 6 [LS, SS, N] [231.0, 1085.0, 59.0] Mean: 3.92, Std Dev: 1.75 ============================= Incoming Item: 1 [LS, SS, N] [232.0, 1086.0, 60.0] Mean: 3.87, Std Dev: 1.77 =============================","title":"Simple Statistics"},{"location":"stream_clusteres/stream_clusterers/","text":"Stream Clustering Algorithms Clustream vs Denstream The Clustream algorithm assumes the clusters are spherical in nature, so it performs poorly when the clusters have arbitrary shapes. Denstream overcomes this limitation since it uses a version of DBSCAN as the offline clustering algorithm. We are going to simulate a stream with 3 clusters consisting of 2-dimensional points. Plotted below is a sample of 500 points from the stream. We can see that there are 2 natural clusters and they are not circular. Clustream After running the Clustream algorithm with 50 microclusters, the points are summarized with microclusters library(\"streamMOA\") stream <- DSD_Gaussians(k=3, d=2, noise=0.05) # cluster with CluStream clustream <- DSC_CluStream(m=50) update(clustream, stream, 500) clustream ## CluStream ## Class: moa/clusterers/clustream/WithKmeans, DSC_Micro, DSC_MOA, DSC ## Number of micro-clusters: 50 ## Number of macro-clusters: 5 # plot micro-clusters plot(clustream, stream) This plot shows the assignment area (radii) of the Micro-Clusters # plot assignment area (micro-cluster radius) plot(clustream, stream, assignment=TRUE, weights=FALSE) In the offline phase, the following clusters are found by K-Means # reclustering. Use weighted k-means for CluStream kmeans <- DSC_Kmeans(k=3, weighted=TRUE) recluster(kmeans, clustream) plot(kmeans, stream, type=\"both\") Denstream Now we will use Denstream to cluster the same set of points. We notice that Clustream has included a few outliers in its final clustering result, wheras Denstream has correctly removed outliers, and was also able to find arbitrary shaped clusters. denstream <- DSC_DenStream(epsilon=0.05) update(denstream, stream, 500) plot(denstream, stream, type=\"both\")","title":"Stream clusterers"},{"location":"stream_clusteres/stream_clusterers/#stream-clustering-algorithms","text":"","title":"Stream Clustering Algorithms"},{"location":"stream_clusteres/stream_clusterers/#clustream-vs-denstream","text":"The Clustream algorithm assumes the clusters are spherical in nature, so it performs poorly when the clusters have arbitrary shapes. Denstream overcomes this limitation since it uses a version of DBSCAN as the offline clustering algorithm. We are going to simulate a stream with 3 clusters consisting of 2-dimensional points. Plotted below is a sample of 500 points from the stream. We can see that there are 2 natural clusters and they are not circular.","title":"Clustream vs Denstream"},{"location":"stream_clusteres/stream_clusterers/#clustream","text":"After running the Clustream algorithm with 50 microclusters, the points are summarized with microclusters library(\"streamMOA\") stream <- DSD_Gaussians(k=3, d=2, noise=0.05) # cluster with CluStream clustream <- DSC_CluStream(m=50) update(clustream, stream, 500) clustream ## CluStream ## Class: moa/clusterers/clustream/WithKmeans, DSC_Micro, DSC_MOA, DSC ## Number of micro-clusters: 50 ## Number of macro-clusters: 5 # plot micro-clusters plot(clustream, stream) This plot shows the assignment area (radii) of the Micro-Clusters # plot assignment area (micro-cluster radius) plot(clustream, stream, assignment=TRUE, weights=FALSE) In the offline phase, the following clusters are found by K-Means # reclustering. Use weighted k-means for CluStream kmeans <- DSC_Kmeans(k=3, weighted=TRUE) recluster(kmeans, clustream) plot(kmeans, stream, type=\"both\")","title":"Clustream"},{"location":"stream_clusteres/stream_clusterers/#denstream","text":"Now we will use Denstream to cluster the same set of points. We notice that Clustream has included a few outliers in its final clustering result, wheras Denstream has correctly removed outliers, and was also able to find arbitrary shaped clusters. denstream <- DSC_DenStream(epsilon=0.05) update(denstream, stream, 500) plot(denstream, stream, type=\"both\")","title":"Denstream"}]}